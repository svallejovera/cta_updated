# Week 6: Scaling Techniques and Topic Modeling {#week6}

## Slides{.unnumbered}

- 6 Scaling Techniques and Topic Modeling ([link](https://github.com/svallejovera/cta_updated/blob/main/slides/6%20Scaling%20Techniques%20and%20Topic%20Modeling.pptx) to slides) 

## Setup

As always, we first load the packages that we'll be using:

```{r, warning = F, message = F}
library(tidyverse) # for wrangling data
library(tidylog) # to know what we are wrangling
library(tidytext) # for 'tidy' manipulation of text data
library(quanteda) # tokenization power house
library(quanteda.textmodels)
library(stm) # run structural topic models
library(wesanderson) # to prettify
```

We get the data from the inaugural speeches again.

```{r}
us_pres <- readxl::read_xlsx(path = "data/inaugTexts.xlsx")
head(us_pres)
```

The text is pretty clean, so we can convert it into a corpus object, then into a `dfm`:

```{r}
corpus_us_pres <- corpus(us_pres,
                     text_field = "inaugSpeech",
                     unique_docnames = TRUE)

summary(corpus_us_pres)
```

```{r}
# We do the whole tokenization sequence
toks_us_pres <- tokens(corpus_us_pres,
                   remove_numbers = TRUE, # Thinks about this
                   remove_punct = TRUE, # Remove punctuation!
                   remove_url = TRUE) # Might be helpful

toks_us_pres <- tokens_remove(toks_us_pres,
                              # Should we though? See Denny and Spirling (2018)
                              c(stopwords(language = "en")),
                              padding = F)

toks_us_pres <- tokens_wordstem(toks_us_pres, language = "en")

dfm_us_pres <- dfm(toks_us_pres)
```
  
## Structural Topic Models

STM provides two ways to include contextual information to “guide” model estimation. First, **topic prevalence** can vary by metadata (e.g., Republicans talk about military issues more than Democrats). Second, **topic content** can vary by metadata (e.g., Republicans talk about military issues differently from Democrats).

We can run STM using the `stm` package. The package includes a complete workflow (i.e., from raw text to figures), and if you are planning to use it in the future, I highly encourage you to check [this](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf), [this](https://www.jstor.org/stable/pdf/24363543.pdf?casa_token=b_rJjIOUUScAAAAA:KXNQeVBQMzB7-kIEhl-1qo6uyD7vHvRTHhMinMdZVT6G3M3olzKzPv00XMJQd7mRw9Nm9UqJDmWHv3N_0cXBmbdeu2XZv8-jy1RYxvpm7Ab3WEOmApXP), [this](https://juliasilge.com/blog/evaluating-stm/), and [this](https://juliasilge.com/blog/sherlock-holmes-stm/). 

At a high level, `stm()` takes our *dfm* and produces topics. If we do not specify any prevalence terms, it will estimate an LDA-style model. Because this is a Bayesian approach, it is recommended that you set a seed value for replication. We also need to choose $K$, the number of topics. How many topics is the “right” number? There is no good number. Too many pre-specified topics and the categories might be meaningless. Too few, and you might be piling together two or more topics. Note that changes to a) the number of topics, b) the prevalence term, c) the omitted words, d) the seed value, can (greatly) change the outcome. Here is where validation becomes crucial (for a review see [Wilkerson and Casas 2017](https://www.researchgate.net/profile/Andreu_Casas/publication/317140610_Large-Scale_Computerized_Text_Analysis_in_Political_Science_Opportunities_and_Challenges/links/59285e6f0f7e9b9979a35ec4/Large-Scale-Computerized-Text-Analysis-in-Political-Science-Opportunities-and-Challenges.pdf)).

Using our presidential speeches data, I will use `stm` to estimate topics in inaugural addresses. As the prevalence term, I include the party of the speaker. I set the number of topics to 10 (but with a corpus this large, I would likely start around ~30 and work my way up from there).

```{r}
stm_us_pres <- stm(dfm_us_pres, K = 10, seed = 1984,
                   prevalence = ~party,
                   init.type = "Spectral")
```

The nice thing about the `stm()` function is that it allows us to see, in “real time,” what is going on inside the black box. We can summarize the process as follows (this is similar to collapsed Gibbs sampling, which `stm()` sort of uses):

1. Go through each document and randomly assign each word in the document to one of the topics, $\displaystyle t \in k$.

2. Notice that this random assignment already gives topic representations for all documents and word distributions for all topics (albeit not very good ones).

3. To improve these estimates, for each document $\displaystyle W$, do the following:

   3.1 Go through each word $\displaystyle w$ in $\displaystyle W$.

   3.1.1 For each topic $\displaystyle t$, compute two quantities:

   3.1.1.1 $\displaystyle p(t \mid W)$: the proportion of words in document $\displaystyle W$ that are currently assigned to topic $\displaystyle t$; and

   3.1.1.2 $\displaystyle p(w \mid t)$: the proportion of assignments to topic $\displaystyle t$ (across all documents) that come from the word $\displaystyle w$.

   Reassign $\displaystyle w$ to a new topic by choosing topic $\displaystyle t$ with probability $\displaystyle p(t \mid W)\, p(w \mid t)$. Under the generative model, this is essentially the probability that topic $\displaystyle t$ generated word $\displaystyle w$, so it makes sense to resample the current word’s topic using this probability. (I’m glossing over a couple of details here—most notably the use of priors/pseudocounts in these probabilities.)

   3.1.1.3 In other words, at this step we assume that all topic assignments except for the current word are correct, and then update the assignment of the current word using our model of how documents are generated.

4. After repeating the previous step many times, you eventually reach a roughly steady state where the assignments are reasonably good. You can then use these assignments to estimate (a) the topic mixture of each document (by counting the proportion of words assigned to each topic within that document) and (b) the words associated with each topic (by counting the proportion of words assigned to each topic overall).

(This explanation was adapted from [here](https://wiki.ubc.ca/Course:CPSC522/Latent_Dirichlet_Allocation#cite_note-rcode-4).) Let’s explore the topics produced:
                                        
```{r}
labelTopics(stm_us_pres)
```

*FREX* weights words by both their overall frequency and how exclusive they are to the topic. *Lift* weights words by dividing by their frequency in other topics, which gives higher weight to words that appear less frequently elsewhere. Similar to Lift, *Score* divides the log frequency of a word in the topic by the log frequency of that word in other topics [(Roberts et al. 2013)](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf). [Bischof and Airoldi (2012)](https://icml.cc/2012/papers/113.pdf) show the value of using **FREX** over the other measures.
                                
You can use the `plot()` function to show the topics.
 
```{r}
plot(stm_us_pres, type = "summary", labeltype = "frex") # or prob, lift score
```

Topic 5 seems to be about the economy: revenue, tariffs, etc. Topic 3 about slavery adn the Civil War. If you want to see a sample of a specific topic:
   
```{r results='hide'}
findThoughts(stm_us_pres, texts = as.character(corpus_us_pres)[docnames(dfm_us_pres)], topics = 3)  
```

That is a long speech.
 
We can (should/must) run some diagnostics. There are two qualities that were are looking for in our model: semantic coherence and exclusivity. Exclusivity is based on the FREX labeling matrix. Semantic coherence is a criterion developed by Mimno et al. (2011) and it maximizes when the most probable words in a given topic frequently co-occur together. Mimno et al. (2011) show that the metric correlates well with human judgement of topic quality. Yet, it is fairly easy to obtain high semantic coherence so it is important to see it in tandem with exclusivity. Let's see how exclusive are the words in each topic:
 
```{r}
dotchart(exclusivity(stm_us_pres), labels = 1:10)
```

We can also see the semantic coherence of our topics --words a topic generates should co-occur often in the same document--:
 
```{r}
dotchart(semanticCoherence(stm_us_pres,dfm_us_pres), labels = 1:10)
```
 
We can also see the overall quality of our topic model:

```{r Quality}
topicQuality(stm_us_pres,dfm_us_pres)
```

On their own, both metrics are not really useful (what do those numbers even mean?). They are useful when we are looking for the "optimal" number of topics. 

```{r, results='hide', message=FALSE}
stm_us_pres_10_15_20 <- manyTopics(dfm_us_pres,
                       prevalence = ~ party,
                       K = c(10,15,20), runs=2,
                       # max.em.its = 100, 
                       init.type = "Spectral") # It takes around 250 iterations for the model to converge. Depending on your computer, this can take a while.

```

We can now compare the performance of each model based on their semantic coherence and exclusivity. We are looking for high exclusivity and high coherence (top-right corner): 


```{r }
k_10 <- stm_us_pres_10_15_20$out[[1]] # k_10 is an stm object which can be explored and used like any other topic model. 
k_15 <- stm_us_pres_10_15_20$out[[2]]
k_20 <- stm_us_pres_10_15_20$out[[3]]

# I will just graph the 'quality' of each model:
topicQuality(k_10,dfm_us_pres)
topicQuality(k_15,dfm_us_pres)
topicQuality(k_20,dfm_us_pres)

```

Maybe we have some theory about the difference in topic prevalence across parties. We can see the topic proportions in our topic model object:
 
```{r}
head(stm_us_pres$theta)
```

Note that the prevalence terms $\theta$ will add to 1 within a document. That is, the term tells us the proportion of (words associated with) topics for each document:

```{r}
sum(stm_us_pres$theta[1,])
sum(stm_us_pres$theta[2,])
```

What about connecting this info to our dfm and seeing if there are differences in the proportion topic 5 (economy) is addressed by each side. 
 
```{r}
library(fixest)
library(sjPlot)

us_pres_prev <- data.frame(topic5 = stm_us_pres$theta[,5], docvars(dfm_us_pres))
feols_topic5 <- feols(topic5 ~ party , data = us_pres_prev)
plot_model(feols_topic5, type = "pred", term = "party") +
  theme_minimal() +
  labs(caption = "Stat. Sig. at p<0.1", x="", y="Topic Prevalence")
```

Seems that Republican presidents address more the economy in their speeches. Let's plot the proportion of speeches about the economy by president:

```{r}
us_pres_prev %>%
  # Going to log the prev of topic 5 because is quite skewed but you should probably leave as is if you want to explore how topics are addressed. 
  ggplot(aes(x = log(topic5), y = reorder(President,topic5), color = party)) +
  geom_point(alpha = 0.8) +
  labs(x = "log(Theta)", y = "", color = "Party") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme_minimal() 
```

We can do something similar with the `stm` function directly. We just need to specify the functional form and add the document variables. 
 
```{r}
topics_us_pres <- estimateEffect(c(3,5) ~ party, stm_us_pres, docvars(dfm_us_pres)) # You can compare other topics by changing c(6,9). 
plot(topics_us_pres, "party", method = "difference",
     cov.value1 = "Democrat", 
     cov.value2 = "Republican",
     labeltype = "custom",
     xlim = c(-.75,.25),
     custom.labels = c('Topic 3: Slavery', 'Topic 5: Economy'),
     model = stm_us_pres)
``` 

Same results, Republicans mention more Topic 5: Economy.

# Week 1: A Primer on Using Text as Data {#week1}

## Slides{.unnumbered}

- 1 Introduction to CTA ([link](https://github.com/svallejovera/cta_updated/blob/main/slides/1%20Introduction%20to%20CTA.pptx)
- 2 Why Computational Text Analysis? ([link](https://github.com/svallejovera/cpa_uwo/blob/main/slides/2%20Why%20Computational%20Text%20Analysis.pptx)

## Setup

For this first example, we will replicate (and extend) Mendenhall's (1887) and Mendenhall's (1901) studies of word-length distribution. 

```{r curve, echo=FALSE,fig.align = 'center', out.width = "65%", fig.cap = "From Mendenhall (1987) - The Characteristic Curves of Composition."}
knitr::include_graphics("images/curve.png")
```

First we load the packages that we'll be using:

```{r, warning = F, message = F}
library(tidyverse) # for wrangling data
library(tidylog) # to know what we are wrangling
library(tidytext) # for 'tidy' manipulation of text data
library(wesanderson) # to prettify
library(gutenbergr) # to get some books
library(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)
```

## Get Data

Mendenhall (1887) argued that “every writer makes use of a vocabulary which is peculiar to himself, and the character of which does not materially change from year to year during his productive [years],” and that one of these characteristics was word length. Mendenhall (1901) takes this further and suggests that, given this assumption, Shakespeare and Bacon were *not* the same person[^2].

Let’s get a corpus--a collection of documents--that we can analyze. We can search the Gutenberg repository and create a corpus with selected works.

```{r, warning = F, message = F}
# Inspect Project Gutenberg metadata to find all texts authored by Oscar Wilde.
# This returns a table of matching Gutenberg entries (including IDs you can use to download texts).
gutenberg_metadata %>%
  filter(author == "Wilde, Oscar")
```

## Word Length in Wilde's Corpus

That’s a lot of Wilde! Let’s focus on four plays: "The Importance of Being Earnest", "A Woman of No Importance", "Lady Windermere's Fan", and "An Ideal Husband". We can download all of these plays using their Gutenberg ID numbers:

```{r, warning = F, message = F}
# Download four Oscar Wilde plays from Project Gutenberg using their Gutenberg IDs.
# The IDs correspond to specific texts in the Gutenberg catalog.
# `meta_fields` appends the requested metadata (here: title and author) to each row of text.
wilde <- gutenberg_download(
  c(790, 844, 854, 885),
  meta_fields = c("title", "author")
)

# Quick inspection: print rows 51–75 (often useful for seeing the structure of the raw text)
# and show up to 25 rows in the console output.
print(n = 25, wilde[c(51:75), ])
```

In this case, the unit of analysis is something like a line. We are interested in each word--also known as a token--and its length *within each play*. We will clean some unwanted text--text that would only add noise to our analysis--and then count the number of words.

```{r}
wilde <- wilde %>%
  # Standardize the title for "The Importance of Being Earnest"
  # (Gutenberg titles can vary slightly across editions/records).
  mutate(
    title = ifelse(
      str_detect(title, "Importance of Being"),
      "The Importance of Being Earnest",
      title
    )
  ) %>%
  # Remove empty lines (blank rows add noise and can affect tokenization/counts).
  filter(text != "") %>%
  # Remove speaker labels typical of plays (often written in ALL CAPS).
  # This keeps primarily spoken text rather than character-name headers.
  filter(str_detect(text, "[A-Z]{3,}") == FALSE)

# Inspect a slice of the cleaned text to confirm the filters behaved as expected.
print(n = 25, wilde[c(51:75), ])
```

Now, we can change our unit of analysis to the **token**:

```{r}
wilde_words <- wilde %>%
  # Tokenize: split the `text` column into one word per row.
  # The output column is named `word`; punctuation is removed and words are lowercased by default.
  unnest_tokens(word, text) %>%
  # Remove underscores (some Gutenberg texts include formatting artifacts like "_" that add noise).
  mutate(word = str_remove_all(word, "\\_"))

# View the tokenized dataset (one row per token, with title/author carried along).
wilde_words

```

That's a lot of words! We will now create a column for word length, and then count the number of words by length (by play!). 

```{r}
wilde_words_ct <- wilde_words %>%
  # Compute the length (number of characters) of each token
  mutate(word_length = str_length(word)) %>%
  # Group by play title and word length to build the word-length distribution
  group_by(word_length, title) %>%
  # Count how many tokens fall into each (word_length, title) bin
  # (n() returns the group size; mutate repeats it on every row in the group)
  mutate(total_word_length = n()) %>%
  # Keep a single row per (word_length, title) combination
  distinct(word_length, title, .keep_all = TRUE) %>%
  # Keep only the variables needed for plotting/inspection
  select(word_length, title, author, total_word_length)

```

Let's see the distribution of word length by play:

```{r}
wilde_words_ct %>%
  # Plot the word-length distribution for each play
  ggplot(aes(x = word_length, y = total_word_length, color = title)) +
  # Points show observed counts at each word length
  geom_point(alpha = 0.8) +
  # Lines connect points to make the distribution shape easier to see
  geom_line(alpha = 0.8) +
  # Use a Wes Anderson palette for play colors
  scale_color_manual(values = wes_palette("Royal2")) +
  # Clean, minimal theme
  theme_minimal() +
  # Place legend on the right for readability
  theme(legend.position = "right") +
  # Axis labels (x = word length in characters; y = number of tokens of that length)
  labs(x = "Length", y = "Total Number of Words", color = "")
```

This is a problem. *Why?*

Here is a solution (proposed by Mendenhall):

```{r}
wilde_words %>%
  # Work within each play separately
  group_by(title) %>%
  # Take an equal-sized random sample of tokens from each play
  # (this makes the resulting distributions comparable across plays)
  slice_sample(n = 10000) %>%
  # Compute word length for each token, and the median word length within each play (on the sampled data)
  mutate(
    word_length = str_length(word),
    median_word_length = median(word_length)
  ) %>%
  # Count how many sampled tokens fall into each word-length bin, within each play
  group_by(word_length, title) %>%
  mutate(total_word_length = n()) %>%
  # Keep one row per (word_length, title) combination for plotting
  distinct(word_length, title, .keep_all = TRUE) %>%
  # Keep relevant columns (median_word_length is repeated but useful for plotting the median line)
  select(word_length, title, author, total_word_length, median_word_length) %>%
  # Plot the sampled word-length distributions
  ggplot(aes(x = word_length, y = total_word_length, color = title)) +
  geom_point(alpha = 0.8) +
  geom_line(alpha = 0.8) +
  # Add a vertical line at each play's median word length
  geom_vline(aes(xintercept = median_word_length, color = title, linetype = title)) +
  scale_color_manual(values = wes_palette("Royal2")) +
  theme_minimal() +
  theme(legend.position = "right") +
  labs(
    x = "Length",
    y = "Total Number of Words",
    color = "",
    linetype = "",
    caption = "Note: Line type shows median word length."
  )
```

Would you look at that, Mendenhall was onto something: an author may have a *signature* in terms of word-length distribution. For Wilde, there is no obvious change across time (each play was published in a different year). But what happens when we compare Wilde’s signature with Shakespeare’s? Let’s choose four plays (at random) by Shakespeare: *A Midsummer Night’s Dream*, *The Merchant of Venice*, *Much Ado About Nothing*, and *The Tempest*.

## Comparing Shakespeare and Wilde

```{r}
shakes <- gutenberg_download(c(1520,2242,2243,2235),
                             meta_fields = c("title","author"))
print(n=25,shakes[c(51:75),])
```

This text is cleaner than Wilde's corpus, so we will leave it as is. Also, it is harder to systematically remove the name of the person speaking. **Is this a problem? Why? Why not?** 

We can put together both corpora and see differences in the distributions of word length. 

```{r}
shakes_words <- shakes %>%
  # Filter out all empty rows
  filter(text != "") %>%
  # This is a play. The name of each character before they speak 
  filter(str_detect(text,"[A-Z]{3,}")==FALSE) %>%
  # take the column text and divide it by words
  unnest_tokens(word, text) 

# Bind both word dfs
words <- rbind.data.frame(shakes_words,wilde_words)

# Count words etc.
words %>%
  group_by(title,author) %>%
  slice_sample(n=10000) %>%
  mutate(word_length = str_length(word),
         median_word_length = median(word_length)) %>%
  group_by(word_length,title,author) %>%
  mutate(total_word_length = n()) %>%
  distinct(word_length,title,.keep_all=T) %>%
  select(word_length,title,author,total_word_length,median_word_length) %>%
  ggplot(aes(y=total_word_length,x=word_length,color=author,group=title)) +
  geom_point(alpha=0.8) +
  geom_line(alpha=0.8) +
  scale_color_manual(values = wes_palette("Royal2")) +
  # facet_wrap(~author, ncol = 2)+
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(x="Length", y = "Total Number of Words", color = "", linetype = "",
       caption = "Note: Median word length is 4 for both authors.")
```

Are there any differences? What can we conclude from the evidence? What are the limitations of this approach? Are there alternative approaches to study what Mendenhall was getting at?

## Exercise (Optional)

1. Extend the current analysis to other authors or to more works by the same author.
2. Are there better ways to compare the distribution of word length? Are there changes across time? Are there differences between different types of works (e.g., fiction vs. non-fiction, prose vs. poetry)?

## Final Words

As will often be the case, we won’t be able to cover every single feature that the different packages have to offer, show every object we create, or explore everything we can do with them. My advice is that you go home and explore the code in detail. Try applying it to a different corpus and come to the next class with questions (or just show off what you were able to do).

[^2]: These studies were published in *Science* and *Popular Science Monthly*!



# Week 5: Scaling Techniques (Unsupervised Learning I) {#week5}

## Slides{.unnumbered}

- 6 Scaling Techniques and Topic Modeling ([link](https://github.com/svallejovera/cta_updated/blob/main/slides/6%20Scaling%20Techniques%20and%20Topic%20Modeling.pptx) to slides) 

## Setup

As always, we first load the packages that we'll be using:

```{r, warning = F, message = F}
# devtools::install_github("conjugateprior/austin")
library(austin) # just for those sweet wordscores
library(tidyverse) # for wrangling data
library(tidylog) # to know what we are wrangling
library(tidytext) # for 'tidy' manipulation of text data
library(quanteda) # tokenization power house
library(quanteda.textmodels)
library(stm) # run structural topic models
library(wesanderson) # to prettify
```

## Wordscores

Laver et al. (2003) propose a supervised scaling technique called *Wordscores*. We learned the intuition in this week's lecture. We will now replicate Table 1 from Laver and Benoit (2003) using the `austin` package. The package includes sample data that we will use:

```{r}
data(lbg)
```

Let's keep only the reference documents:

```{r}
ref <- getdocs(lbg, 1:5)
ref
```

This is the same matrix as in Figure 1, where we have a count of each word (in this case, letters) by reference document (i.e., documents that have already been labeled). We can assign scores (`A_scores`) to each reference text to place them on an ideological scale (or whatever scale we want). We then estimate *Wordscores* for each word.

```{r}
# We do this in the order of the reference texts:
A_score <- c(-1.5,-0.75,0,0.75,1.5)
ws <- classic.wordscores(ref, scores=A_score)
ws$pi
```

Now we get the virgin text and predict the textscore by estimating the average of the weighted wordscores for the virgin document:

```{r}
vir <- getdocs(lbg, 'V1')
vir
```

```{r}
# predict textscores for the virgin documents
predict(ws, newdata=vir)
```

Cool. 

## Wordfish

Slapin and Proksch (2008) propose an unsupervised scaling model that places texts on a one-dimensional scale. The underlying assumption is that

$$w_{ik} \sim \text{Poisson}(\lambda_{ik})$$
$$\lambda_{ik} = \exp(\alpha_i + \psi_k + \beta_k \times \theta_i)$$

Here, $\lambda_{ik}$ is generated by $\alpha_i$ (the “loquaciousness” of politician $i$, or document fixed effects), $\psi_k$ (the baseline frequency of word $k$), $\beta_k$ (the discrimination parameter of word $k$), and—most importantly, $\theta_i$ (the politician’s ideological position). Let’s believe, for a moment, that the peer-review system works and use the `textmodel_wordfish()` function to estimate the ideological positions of U.S. presidents using their inaugural speeches.

```{r}
us_pres <- readxl::read_xlsx(path = "data/inaugTexts.xlsx")
head(us_pres)
```

The text is pretty clean, so we can convert it into a corpus object, then into a `dfm`, and apply `textmodel_wordfish()`:

```{r}
corpus_us_pres <- corpus(us_pres,
                     text_field = "inaugSpeech",
                     unique_docnames = TRUE)

summary(corpus_us_pres)
```

```{r}
# We do the whole tokenization sequence
toks_us_pres <- tokens(corpus_us_pres,
                   remove_numbers = TRUE, # Thinks about this
                   remove_punct = TRUE, # Remove punctuation!
                   remove_url = TRUE) # Might be helpful

toks_us_pres <- tokens_remove(toks_us_pres,
                              # Should we though? See Denny and Spirling (2018)
                              c(stopwords(language = "en")),
                              padding = F)

toks_us_pres <- tokens_wordstem(toks_us_pres, language = "en")

dfm_us_pres <- dfm(toks_us_pres)

wfish_us_pres <- textmodel_wordfish(dfm_us_pres, dir = c(28,30)) #Does not really matter what the starting values are, they just serve as anchors for the relative position of the rest of the texts. In this case, I chose Kennedy and Nixon.  
summary(wfish_us_pres)
```
  
Let’s see whether this makes any sense. Since we know each president's party, we would expect Republican presidents to cluster together and separate from Democrats (or something along those lines):

```{r}
# Get predictions:
wfish_preds <- predict(wfish_us_pres, interval = "confidence")

# Tidy everything up:
posi_us_pres <- data.frame(docvars(corpus_us_pres),
                      wfish_preds$fit) %>%
  arrange(fit)

# Plot
posi_us_pres %>%
  ggplot(aes(x = fit, y = reorder(President,fit), xmin = lwr, xmax = upr, color = party)) +
  geom_point(alpha = 0.8) +
  geom_errorbarh(height = 0) +
  labs(x = "Position", y = "", color = "Party") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme_minimal() +
  ggtitle("Estimated Positions")
```

Two things to note. First, the *direction* of the scale is a theoretically grounded decision that the researcher has to make (not the algorithm). In our case, based on these results, we could interpret positive values as more left-leaning and negative values as more right-leaning. For visualization purposes, we can flip the direction simply by multiplying by -1:

```{r}
# Plot inverse
posi_us_pres %>%
  ggplot(aes(x = -fit, y = reorder(President,fit), xmin = -lwr, xmax = -upr, color = party)) +
  geom_point(alpha = 0.8) +
  geom_errorbarh(height = 0) +
  labs(x = "Position", y = "", color = "Party") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme_minimal() +
  ggtitle("Estimated Positions")
```

Second, there seems to be a mismatch between our theoretical expectations and our empirical observations. We might assume that Republicans (Democrats) would talk more similarly to other Republicans (Democrats) and differently from Democrats (Republicans). However, that is not what we see here. *What could be happening?*

One possibility is that language changes over time, issues change over time, or even what it means to be a Democrat or Republican changes over time. and the model is picking up that temporal shift:

```{r}
# Plot time
posi_us_pres %>%
  ggplot(aes(y = -fit, x = Year, ymin = -lwr, ymax = -upr, color = party)) +
  geom_point(alpha = 0.8) +
  geom_errorbar() +
  labs(x = "Year", y = "Position", color = "Party") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme_minimal() +
  ggtitle("Estimated Positions")

```

That seems like one possible explanation. Another is that our preprocessing steps substantively modified the texts (see Denny and Spirling 2018). We can estimate the model again using a differently preprocessed version of the text:

```{r}
# Tokenization only removing punctuation
toks_us_pres2 <- tokens(corpus_us_pres,
                   remove_punct = TRUE) 

dfm_us_pres2 <- dfm(toks_us_pres2)
wfish_us_pres <- textmodel_wordfish(dfm_us_pres2, dir = c(28,30))  

# Get predictions:
wfish_preds <- predict(wfish_us_pres, interval = "confidence")

# Tidy everything up:
posi_us_pres <- data.frame(docvars(corpus_us_pres),
                      wfish_preds$fit) %>%
  arrange(fit)

# Plot
posi_us_pres %>%
  ggplot(aes(x = -fit, y = reorder(President,fit), xmin = -lwr, xmax = -upr, color = party)) +
  geom_point(alpha = 0.8) +
  geom_errorbarh(height = 0) +
  labs(x = "Position", y = "", color = "Party") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme_minimal() +
  ggtitle("Estimated Positions (Minimal Pre-Processing)")
```

At the very least, the within president differences in estimates have narrowed, but time seems to still be the best predictor: 

```{r}
# Plot time
posi_us_pres %>%
  ggplot(aes(y = -fit, x = Year, ymin = -lwr, ymax = -upr, color = party)) +
  geom_point(alpha = 0.8) +
  geom_errorbar() +
  labs(x = "Year", y = "Position", color = "Party") +
  scale_color_manual(values = wes_palette("BottleRocket2")) +
  theme_minimal() +
  ggtitle("Estimated Positions (Minimal Pre-Processing)")

```

If time is the main predictor, then we may need to focus on periods that are more comparable across parties (e.g., the era after the Civil Rights Act).

## Homework 2:

1. We had a hard time scaling our text, so we looked at some possible problems. What are possible solutions if we want to position U.S. presidents on an ideological scale using text?
2. Use the `data/candidate-tweets.csv` data to run an STM. Decide what your covariates are going to be. Decide whether you will use all the data or a sample of the data. Decide whether you are going to aggregate or split the text in some way (i.e., decide your unit of analysis). Decide the number of topics you will look for (try more than one option). What can you tell me about the topics tweeted by the 2015 U.S. primary candidates?
3. Choose three topics ([see Week 6](#week6)). Can you place the candidates on an ideological scale within each topic (determine the $\theta$ threshold for when you can say that a tweet is *mostly* about a topic)? Does it make sense? Why or why not?

<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Reading List | PS9594: Computational Text Analysis</title>
<meta name="author" content="Dr. Sebastián Vallejo Vera | Western University">
<meta name="description" content="Week #1: Course Introduction / Why (Computational) Text Analysis? Topics: Review of syllabus and class organization. Introduction to computational text analysis and natural language processing...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Reading List | PS9594: Computational Text Analysis">
<meta property="og:type" content="book">
<meta property="og:description" content="Week #1: Course Introduction / Why (Computational) Text Analysis? Topics: Review of syllabus and class organization. Introduction to computational text analysis and natural language processing...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reading List | PS9594: Computational Text Analysis">
<meta name="twitter:description" content="Week #1: Course Introduction / Why (Computational) Text Analysis? Topics: Review of syllabus and class organization. Introduction to computational text analysis and natural language processing...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">PS9594: Computational Text Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">“Computational Text Analysis”</a></li>
<li><a class="active" href="readings.html">Reading List</a></li>
<li><a class="" href="assignments.html">Assignments</a></li>
<li><a class="" href="replication.html">Replication Exercise</a></li>
<li><a class="" href="final_paper.html">Final Paper</a></li>
<li><a class="" href="week1.html"><span class="header-section-number">1</span> Week 1: A Primer on Using Text as Data</a></li>
<li><a class="" href="week2.html"><span class="header-section-number">2</span> Week 2: Tokenization and Word Frequency</a></li>
<li><a class="" href="week3.html"><span class="header-section-number">3</span> Week 3: Dictionary-Based Approaches</a></li>
<li><a class="" href="week4.html"><span class="header-section-number">4</span> Week 4: Complexity and Similarity</a></li>
<li><a class="" href="week5.html"><span class="header-section-number">5</span> Week 5: Scaling Techniques (Unsupervised Learning I)</a></li>
<li><a class="" href="week6.html"><span class="header-section-number">6</span> Week 6: Scaling Techniques and Topic Modeling</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/svallejovera/cta_updated">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="readings" class="section level1 unnumbered">
<h1>Reading List<a class="anchor" aria-label="anchor" href="#readings"><i class="fas fa-link"></i></a>
</h1>
<div id="week-1-course-introduction-why-computational-text-analysis" class="section level2 unnumbered">
<h2>Week #1: Course Introduction / Why (Computational) Text Analysis?<a class="anchor" aria-label="anchor" href="#week-1-course-introduction-why-computational-text-analysis"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> Review of syllabus and class organization. Introduction to computational text analysis and natural language processing (NLP).</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapter 2.</p></li>
<li><p>Wilkerson, John, and Andreu Casas. 2017. “Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges.” <em>Annual Review of Political Science</em> 20: 529–544. <a href="https://doi.org/10.1146/annurev-polisci-052615-025542" class="uri">https://doi.org/10.1146/annurev-polisci-052615-025542</a></p></li>
<li><p>Macanovic, Ana. 2022. “Text Mining for Social Science: The State and the Future of Computational Text Analysis in Sociology.” <em>Social Science Research</em> 108: 102784. <a href="https://doi.org/10.1016/j.ssresearch.2022.102784" class="uri">https://doi.org/10.1016/j.ssresearch.2022.102784</a></p></li>
<li><p>Barberá, Pablo, and Gonzalo Rivero. 2015. “Understanding the Political Representativeness of Twitter Users.” <em>Social Science Computer Review</em> 33 (6): 712–729. <a href="https://doi.org/10.1177/0894439314558836" class="uri">https://doi.org/10.1177/0894439314558836</a></p></li>
<li><p>Michalopoulos, Stelios, and Melanie Meng Xue. 2021. “Folklore.” <em>The Quarterly Journal of Economics</em> 136 (4): 1993–2046. <a href="https://doi.org/10.1093/qje/qjab003" class="uri">https://doi.org/10.1093/qje/qjab003</a></p></li>
</ul>
</div>
<div id="week-2-tokenization-and-word-frequency" class="section level2 unnumbered">
<h2>Week #2: Tokenization and Word Frequency<a class="anchor" aria-label="anchor" href="#week-2-tokenization-and-word-frequency"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> What is a bag of words? What are tokens? Why should we care about tokens?</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapter 5.</p></li>
<li><p>Ban, Pamela, Alexander Fouirnaies, Andrew B. Hall, and James M. Snyder, Jr. 2019. “How Newspapers Reveal Political Power.” <em>Political Science Research and Methods</em> 7 (4): 661–678. <a href="https://doi.org/10.1017/psrm.2017.43" class="uri">https://doi.org/10.1017/psrm.2017.43</a></p></li>
<li><p>Michel, Jean-Baptiste, et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” <em>Science</em> 331 (6014): 176–182. <a href="https://doi.org/10.1126/science.1199644" class="uri">https://doi.org/10.1126/science.1199644</a></p></li>
<li><p>Bollen, Johan, et al. 2021. “Historical Language Records Reveal a Surge of Cognitive Distortions in Recent Decades.” <em>Proceedings of the National Academy of Sciences</em> 118 (30): e2102061118. <a href="https://doi.org/10.1073/pnas.2102061118" class="uri">https://doi.org/10.1073/pnas.2102061118</a></p></li>
</ul>
</div>
<div id="week-3-dictionary-based-techniques" class="section level2 unnumbered">
<h2>Week #3: Dictionary-Based Techniques<a class="anchor" aria-label="anchor" href="#week-3-dictionary-based-techniques"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> What are dictionaries? Why and when are they useful? What are their limitations?</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapters 15–16.</p></li>
<li><p>Young, Lori, and Stuart Soroka. 2012. “Affective News: The Automated Coding of Sentiment in Political Texts.” <em>Political Communication</em> 29 (2): 205–231. <a href="https://doi.org/10.1080/10584609.2012.671234" class="uri">https://doi.org/10.1080/10584609.2012.671234</a></p></li>
<li><p>Martins, Mauricio D. J. D., and Nicolas Baumard. 2020. “The Rise of Prosociality in Fiction Preceded Democratic Revolutions in Early Modern Europe.” <em>Proceedings of the National Academy of Sciences</em> 117 (46): 28684–28691.</p></li>
<li><p>Ventura, Tiago, Kevin Munger, Katherine T. McCabe, and Keng-Chi Chang. 2021. “Connective Effervescence and Streaming Chat During Political Debates.” <em>Journal of Quantitative Description: Digital Media</em> 1. <a href="https://doi.org/10.51685/jqd.2021.001" class="uri">https://doi.org/10.51685/jqd.2021.001</a></p></li>
</ul>
</div>
<div id="week-4-natural-language-complexity-and-similarity" class="section level2 unnumbered">
<h2>Week #4: Natural Language, Complexity, and Similarity<a class="anchor" aria-label="anchor" href="#week-4-natural-language-complexity-and-similarity"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> How do we evaluate complexity in text? Why should we care about complexity in text? How do we evaluate similarity in text, and why is this useful?</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapters 6–7.</p></li>
<li><p>Spirling, Arthur. 2016. “Democratization and Linguistic Complexity: The Effect of Franchise Extension on Parliamentary Discourse, 1832–1915.” <em>The Journal of Politics</em> 78 (1): 120–136.</p></li>
<li><p>Urman, Aleksandra, Mykola Makhortykh, and Roberto Ulloa. 2022. “The Matter of Chance: Auditing Web Search Results Related to the 2020 US Presidential Primary Elections Across Six Search Engines.” <em>Social Science Computer Review</em> 40 (5): 1323–1339.</p></li>
<li><p>Schoonvelde, Martijn, Anna Brosius, Gijs Schumacher, and Bert N. Bakker. 2019. “Liberals Lecture, Conservatives Communicate: Analyzing Complexity and Ideology in 381,609 Political Speeches.” <em>PLOS ONE</em> 14 (2): e0208450. <a href="https://doi.org/10.1371/journal.pone.0208450" class="uri">https://doi.org/10.1371/journal.pone.0208450</a></p></li>
</ul>
</div>
<div id="week-5-scaling-techniques-unsupervised-learning-i" class="section level2 unnumbered">
<h2>Week #5: Scaling Techniques (Unsupervised Learning I)<a class="anchor" aria-label="anchor" href="#week-5-scaling-techniques-unsupervised-learning-i"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> What is unsupervised learning? What are scaling models, and what can they tell us?</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapters 12–13.</p></li>
<li><p>Slapin, Jonathan B., and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.” <em>American Journal of Political Science</em> 52 (3): 705–722.</p></li>
<li><p>Denny, Matthew J., and Arthur Spirling. 2018. “Text Preprocessing for Unsupervised Learning: Why It Matters, When It Misleads, and What to Do About It.” <em>Political Analysis</em> 26 (2): 168–189.</p></li>
</ul>
</div>
<div id="week-6-topic-modeling-and-clustering-unsupervised-learning-ii" class="section level2 unnumbered">
<h2>Week #6: Topic Modeling and Clustering (Unsupervised Learning II)<a class="anchor" aria-label="anchor" href="#week-6-topic-modeling-and-clustering-unsupervised-learning-ii"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> What is topic modeling, and what can it tell us?</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapters 12–13.</p></li>
<li><p>Roberts, Margaret E., et al. 2014. “Structural Topic Models for Open-Ended Survey Responses.” <em>American Journal of Political Science</em> 58 (4): 1064–1082.</p></li>
<li><p>Motolinia, Lucia 2021. “Electoral Accountability and Particularistic Legislation: Evidence from an Electoral Reform in Mexico.” <em>American Political Science Review</em> 115 (1): 97–113.</p></li>
</ul>
</div>
<div id="spring-reading-week" class="section level2 unnumbered">
<h2>Spring reading week<a class="anchor" aria-label="anchor" href="#spring-reading-week"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> Enjoy the break!</p>
</div>
<div id="week-7-a-primer-on-supervised-learning" class="section level2 unnumbered">
<h2>Week #7: A Primer on Supervised Learning<a class="anchor" aria-label="anchor" href="#week-7-a-primer-on-supervised-learning"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> What is supervised learning? We will study the framework for training supervised models and when to use them. We will learn how Support Vector Machine (SVM) and Bidirectional Long-Short Term Memory (Bi-LSTM) models work.</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapters 17–20.</p></li>
<li><p>Siegel, Alexandra A., et al. 2021. “Trumping Hate on Twitter? Online Hate Speech in the 2016 US Election Campaign and Its Aftermath.” <em>Quarterly Journal of Political Science</em> 16 (1): 71–104.</p></li>
<li><p>Barberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, and Jonathan Nagler. 2021. “Automated Text Classification of News Articles: A Practical Guide.” <em>Political Analysis</em> 29 (1): 19–42. <a href="https://doi.org/10.1017/pan.2020.8" class="uri">https://doi.org/10.1017/pan.2020.8</a></p></li>
<li><p>Laver, Michael, Kenneth Benoit, and John Garry. 2003. “Extracting Policy Positions from Political Texts Using Words as Data.” <em>American Political Science Review</em> 97 (2): 311–331.</p></li>
<li><p>Benoit, Kenneth, et al. 2016. “Crowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data.” <em>American Political Science Review</em> 110 (2): 278–295.</p></li>
</ul>
</div>
<div id="week-8-introduction-to-deep-learning-and-word-embeddings" class="section level2 unnumbered">
<h2>Week #8: Introduction to Deep Learning and Word Embeddings<a class="anchor" aria-label="anchor" href="#week-8-introduction-to-deep-learning-and-word-embeddings"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> How can we capture the meaning of words? We will use deep learning models to represent text.</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. <em>Text as Data: A New Framework for Machine Learning and the Social Sciences</em>. Princeton, NJ: Princeton University Press. Chapter 8.</p></li>
<li><p>Lin, Gechun, and Christopher Lucas. 2023. “An Introduction to Neural Networks for the Social Sciences.” In <em>The Oxford Handbook of Engaged Methodological Pluralism in Political Science</em>, edited by Janet M. Box-Steffensmeier, Dino P. Christenson, and Valeria Sinclair-Chapman. Oxford: Oxford University Press. <a href="https://doi.org/10.1093/oxfordhb/9780192868282.013.79" class="uri">https://doi.org/10.1093/oxfordhb/9780192868282.013.79</a></p></li>
<li><p>Meyer, David. 2016. “How Exactly Does word2vec Work?” <a href="https://davidmeyer.github.io/ml/how_does_word2vec_work.pdf" class="uri">https://davidmeyer.github.io/ml/how_does_word2vec_work.pdf</a></p></li>
<li><p>Alammar, Jay. 2019. “The Illustrated Word2vec.” <a href="https://jalammar.github.io/illustrated-word2vec/" class="uri">https://jalammar.github.io/illustrated-word2vec/</a></p></li>
<li><p>Rodriguez, Pedro L., and Arthur Spirling. 2022. “Word Embeddings: What Works, What Doesn’t, and How to Tell the Difference for Applied Research.” <em>The Journal of Politics</em> 84 (1): 101–115. <a href="https://doi.org/10.1086/715162" class="uri">https://doi.org/10.1086/715162</a></p></li>
<li><p>Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” <em>American Sociological Review</em> 84 (5): 905–949.</p></li>
</ul>
</div>
<div id="week-9-the-transformers-architecture" class="section level2 unnumbered">
<h2>Week #9: The Transformers Architecture<a class="anchor" aria-label="anchor" href="#week-9-the-transformers-architecture"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> We will learn about the Transformer architecture, attention, and the encoder-decoder framework.</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Alammar, Jay. 2018. “The Illustrated Transformer.” <a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a></p></li>
<li><p>Vaswani, Ashish, et al. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em> 30.</p></li>
<li><p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” <em>arXiv</em> preprint arXiv:1810.04805.</p></li>
<li><p>Timoneda, Joan C., and Sebastián Vallejo Vera. 2025. “BERT, RoBERTa, or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text.” <em>The Journal of Politics</em> 87 (1): 347–364. <a href="https://doi.org/10.1086/730737" class="uri">https://doi.org/10.1086/730737</a></p></li>
</ul>
</div>
<div id="week-10-encoder-only-llms" class="section level2 unnumbered">
<h2>Week #10: Encoder-Only LLMs<a class="anchor" aria-label="anchor" href="#week-10-encoder-only-llms"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> We will take a deep dive into encoder-only LLMs and what we can do with them.</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Taylor, Wilson L. 1953. “ ‘Cloze Procedure’: A New Tool for Measuring Readability.” <em>Journalism Quarterly</em> 30 (4): 415–433.</p></li>
<li><p>Dávila Gordillo, Diana, Joan C. Timoneda, and Sebastián Vallejo Vera. Forthcoming. “Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora.” <em>Sociological Methods &amp; Research</em>. arXiv:2401.09333.</p></li>
</ul>
</div>
<div id="week-11-decoder-only-llms" class="section level2 unnumbered">
<h2>Week #11: Decoder-Only LLMs<a class="anchor" aria-label="anchor" href="#week-11-decoder-only-llms"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Topics:</strong> Decoder-only LLMs, also known as generative LLMs, are all the rage right now. We will study how they work, what they can do, what their limitations are, and how we can use them in our work more broadly.</p>
<p><strong>Readings:</strong></p>
<ul>
<li><p>Lee, Kyuwon, Simone Paci, Jeongmin Park, Hye Young You, and Sylvan Zheng. 2025. “Applications of GPT in Political Science Research: Extracting Information from Unstructured Text.” <em>PS: Political Science &amp; Politics</em> 58 (4): 1–11. <a href="https://doi.org/10.1017/S1049096525000046" class="uri">https://doi.org/10.1017/S1049096525000046</a></p></li>
<li><p>Gilardi, Fabrizio, Meysam Alizadeh, and Maël Kubli. 2023. “ChatGPT Outperforms Crowd Workers for Text-Annotation Tasks.” <em>Proceedings of the National Academy of Sciences</em> 120 (30): e2305016120. <a href="https://doi.org/10.1073/pnas.2305016120" class="uri">https://doi.org/10.1073/pnas.2305016120</a></p></li>
<li><p>Heseltine, Michael, and Bernhard Clemm von Hohenberg. 2024. “Large Language Models as a Substitute for Human Experts in Annotating Political Text.” <em>Research &amp; Politics</em> 11 (1): 20531680241236239. <a href="https://doi.org/10.1177/20531680241236239" class="uri">https://doi.org/10.1177/20531680241236239</a></p></li>
<li><p>Vallejo Vera, Sebastián, and Hunter Driggers. 2025. “LLMs as Annotators: The Effect of Party Cues on Labelling Decisions by Large Language Models.” <em>Humanities and Social Sciences Communications</em> 12: Article 1530. <a href="https://doi.org/10.1057/s41599-025-05834-4" class="uri">https://doi.org/10.1057/s41599-025-05834-4</a></p></li>
<li><p>Walker, Christina P., and Joan C. Timoneda. 2025. “Is ChatGPT Conservative or Liberal? A Novel Approach to Assess Ideological Stances and Biases in Generative LLMs.” <em>Political Science Research and Methods</em> 1–15. <a href="https://doi.org/10.1017/psrm.2025.10057" class="uri">https://doi.org/10.1017/psrm.2025.10057</a></p></li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="index.html">“Computational Text Analysis”</a></div>
<div class="next"><a href="assignments.html">Assignments</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#readings">Reading List</a></li>
<li><a class="nav-link" href="#week-1-course-introduction-why-computational-text-analysis">Week #1: Course Introduction / Why (Computational) Text Analysis?</a></li>
<li><a class="nav-link" href="#week-2-tokenization-and-word-frequency">Week #2: Tokenization and Word Frequency</a></li>
<li><a class="nav-link" href="#week-3-dictionary-based-techniques">Week #3: Dictionary-Based Techniques</a></li>
<li><a class="nav-link" href="#week-4-natural-language-complexity-and-similarity">Week #4: Natural Language, Complexity, and Similarity</a></li>
<li><a class="nav-link" href="#week-5-scaling-techniques-unsupervised-learning-i">Week #5: Scaling Techniques (Unsupervised Learning I)</a></li>
<li><a class="nav-link" href="#week-6-topic-modeling-and-clustering-unsupervised-learning-ii">Week #6: Topic Modeling and Clustering (Unsupervised Learning II)</a></li>
<li><a class="nav-link" href="#spring-reading-week">Spring reading week</a></li>
<li><a class="nav-link" href="#week-7-a-primer-on-supervised-learning">Week #7: A Primer on Supervised Learning</a></li>
<li><a class="nav-link" href="#week-8-introduction-to-deep-learning-and-word-embeddings">Week #8: Introduction to Deep Learning and Word Embeddings</a></li>
<li><a class="nav-link" href="#week-9-the-transformers-architecture">Week #9: The Transformers Architecture</a></li>
<li><a class="nav-link" href="#week-10-encoder-only-llms">Week #10: Encoder-Only LLMs</a></li>
<li><a class="nav-link" href="#week-11-decoder-only-llms">Week #11: Decoder-Only LLMs</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/svallejovera/cta_updated/blob/master/01-readings.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/svallejovera/cta_updated/edit/master/01-readings.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>PS9594: Computational Text Analysis</strong>" was written by Dr. Sebastián Vallejo Vera | Western University. It was last built on 2026-01-25.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

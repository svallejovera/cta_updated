[{"path":"index.html","id":"computational-text-analysis","chapter":"“Computational Text Analysis”","heading":"“Computational Text Analysis”","text":" Welcome site course PS9594A: “Computational Text Analysis” Western University, taught Sebastián Vallejo Vera. week, find lecture slides, lecture code, exercises, code corresponding topic.start, don’t forget read Syllabus check Perusall course readings. site corrected updated throughout semester.","code":""},{"path":"index.html","id":"course-overview","chapter":"“Computational Text Analysis”","heading":"Course Overview","text":"One abundant sources data available social political scientists today text. Recent advances Natural Language Processing (NLP) spearheaded text--data revolution, leading social scientists seek new ways analyze text data scale. course, learn intuition behind–implement–different computational methods process, analyze, classify text. course cover Bag--Words (BoW) approaches, unsupervised methods, supervised semi-supervised methods, LLM-based approaches, well interpret results obtained applying methods.","code":""},{"path":"index.html","id":"readings-assignments-and-final-exam","chapter":"“Computational Text Analysis”","heading":"Readings, Assignments, and Final Exam","text":"can check complete week--week reading list . Note complete texts can found course’s Perusall page.can check complete week--week reading list . Note complete texts can found course’s Perusall page.can check schedule assignments, well instructions presentation submission, .can check schedule assignments, well instructions presentation submission, .can check instructions replication exercise .can check instructions replication exercise .can check instructions final paper exercise .can check instructions final paper exercise .","code":""},{"path":"index.html","id":"software-and-packages","chapter":"“Computational Text Analysis”","heading":"Software and Packages","text":"first part course (Weeks 1 - 5), mainly using R. second part course (Weeks 6 - 11), use combination R Python. assume familiar R language, RStudio, R packages. R, main packages need installed:tidyverse (piping)tidylog (helps keep track piping)tidytext (great working text)quanteda (stands “Quantitative Analysis Textual Data”)\nquanteda.textstats (obtain stats dfm)\nquanteda.textplots (obtain plots dfm stats)\nquanteda.dictionaries (use dictionaries quanteda)\nquanteda.textstats (obtain stats dfm)quanteda.textplots (obtain plots dfm stats)quanteda.dictionaries (use dictionaries quanteda)gutenbergr (download texts Project Gutenberg)wesanderson (make things pretty)stm (run Structural Topic Models)","code":""},{"path":"index.html","id":"acknowledgments","chapter":"“Computational Text Analysis”","heading":"Acknowledgments","text":"organization first part course (Weeks 1 - 5) format assignments borrowed many sources, among Christopher Barrie’s excellent course “Computational Text Analysis”, syllabus Tiago Ventura, Grimmer, Roberts, Stewart’s excellent book, “Text data: new framework machine learning social sciences”. code used throughout course patchwork code, code borrows heavily internet (’s true code). try best give credit original authors code (possible).","code":""},{"path":"readings.html","id":"readings","chapter":"Reading List","heading":"Reading List","text":"","code":""},{"path":"readings.html","id":"week-1-course-introduction-why-computational-text-analysis","chapter":"Reading List","heading":"Week #1: Course Introduction / Why (Computational) Text Analysis?","text":"Topics: Review syllabus class organization. Introduction computational text analysis natural language processing (NLP).Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapter 2.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapter 2.Wilkerson, John, Andreu Casas. 2017. “Large-Scale Computerized Text Analysis Political Science: Opportunities Challenges.” Annual Review Political Science 20: 529–544. https://doi.org/10.1146/annurev-polisci-052615-025542Wilkerson, John, Andreu Casas. 2017. “Large-Scale Computerized Text Analysis Political Science: Opportunities Challenges.” Annual Review Political Science 20: 529–544. https://doi.org/10.1146/annurev-polisci-052615-025542Macanovic, Ana. 2022. “Text Mining Social Science: State Future Computational Text Analysis Sociology.” Social Science Research 108: 102784. https://doi.org/10.1016/j.ssresearch.2022.102784Macanovic, Ana. 2022. “Text Mining Social Science: State Future Computational Text Analysis Sociology.” Social Science Research 108: 102784. https://doi.org/10.1016/j.ssresearch.2022.102784Barberá, Pablo, Gonzalo Rivero. 2015. “Understanding Political Representativeness Twitter Users.” Social Science Computer Review 33 (6): 712–729. https://doi.org/10.1177/0894439314558836Barberá, Pablo, Gonzalo Rivero. 2015. “Understanding Political Representativeness Twitter Users.” Social Science Computer Review 33 (6): 712–729. https://doi.org/10.1177/0894439314558836Michalopoulos, Stelios, Melanie Meng Xue. 2021. “Folklore.” Quarterly Journal Economics 136 (4): 1993–2046. https://doi.org/10.1093/qje/qjab003Michalopoulos, Stelios, Melanie Meng Xue. 2021. “Folklore.” Quarterly Journal Economics 136 (4): 1993–2046. https://doi.org/10.1093/qje/qjab003","code":""},{"path":"readings.html","id":"week-2-tokenization-and-word-frequency","chapter":"Reading List","heading":"Week #2: Tokenization and Word Frequency","text":"Topics: bag words? tokens? care tokens?Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapter 5.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapter 5.Ban, Pamela, Alexander Fouirnaies, Andrew B. Hall, James M. Snyder, Jr. 2019. “Newspapers Reveal Political Power.” Political Science Research Methods 7 (4): 661–678. https://doi.org/10.1017/psrm.2017.43Ban, Pamela, Alexander Fouirnaies, Andrew B. Hall, James M. Snyder, Jr. 2019. “Newspapers Reveal Political Power.” Political Science Research Methods 7 (4): 661–678. https://doi.org/10.1017/psrm.2017.43Michel, Jean-Baptiste, et al. 2011. “Quantitative Analysis Culture Using Millions Digitized Books.” Science 331 (6014): 176–182. https://doi.org/10.1126/science.1199644Michel, Jean-Baptiste, et al. 2011. “Quantitative Analysis Culture Using Millions Digitized Books.” Science 331 (6014): 176–182. https://doi.org/10.1126/science.1199644Bollen, Johan, et al. 2021. “Historical Language Records Reveal Surge Cognitive Distortions Recent Decades.” Proceedings National Academy Sciences 118 (30): e2102061118. https://doi.org/10.1073/pnas.2102061118Bollen, Johan, et al. 2021. “Historical Language Records Reveal Surge Cognitive Distortions Recent Decades.” Proceedings National Academy Sciences 118 (30): e2102061118. https://doi.org/10.1073/pnas.2102061118","code":""},{"path":"readings.html","id":"week-3-dictionary-based-techniques","chapter":"Reading List","heading":"Week #3: Dictionary-Based Techniques","text":"Topics: dictionaries? useful? limitations?Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 15–16.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 15–16.Young, Lori, Stuart Soroka. 2012. “Affective News: Automated Coding Sentiment Political Texts.” Political Communication 29 (2): 205–231. https://doi.org/10.1080/10584609.2012.671234Young, Lori, Stuart Soroka. 2012. “Affective News: Automated Coding Sentiment Political Texts.” Political Communication 29 (2): 205–231. https://doi.org/10.1080/10584609.2012.671234Martins, Mauricio D. J. D., Nicolas Baumard. 2020. “Rise Prosociality Fiction Preceded Democratic Revolutions Early Modern Europe.” Proceedings National Academy Sciences 117 (46): 28684–28691.Martins, Mauricio D. J. D., Nicolas Baumard. 2020. “Rise Prosociality Fiction Preceded Democratic Revolutions Early Modern Europe.” Proceedings National Academy Sciences 117 (46): 28684–28691.Ventura, Tiago, Kevin Munger, Katherine T. McCabe, Keng-Chi Chang. 2021. “Connective Effervescence Streaming Chat Political Debates.” Journal Quantitative Description: Digital Media 1. https://doi.org/10.51685/jqd.2021.001Ventura, Tiago, Kevin Munger, Katherine T. McCabe, Keng-Chi Chang. 2021. “Connective Effervescence Streaming Chat Political Debates.” Journal Quantitative Description: Digital Media 1. https://doi.org/10.51685/jqd.2021.001","code":""},{"path":"readings.html","id":"week-4-natural-language-complexity-and-similarity","chapter":"Reading List","heading":"Week #4: Natural Language, Complexity, and Similarity","text":"Topics: evaluate complexity text? care complexity text? evaluate similarity text, useful?Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 6–7.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 6–7.Spirling, Arthur. 2016. “Democratization Linguistic Complexity: Effect Franchise Extension Parliamentary Discourse, 1832–1915.” Journal Politics 78 (1): 120–136.Spirling, Arthur. 2016. “Democratization Linguistic Complexity: Effect Franchise Extension Parliamentary Discourse, 1832–1915.” Journal Politics 78 (1): 120–136.Urman, Aleksandra, Mykola Makhortykh, Roberto Ulloa. 2022. “Matter Chance: Auditing Web Search Results Related 2020 US Presidential Primary Elections Across Six Search Engines.” Social Science Computer Review 40 (5): 1323–1339.Urman, Aleksandra, Mykola Makhortykh, Roberto Ulloa. 2022. “Matter Chance: Auditing Web Search Results Related 2020 US Presidential Primary Elections Across Six Search Engines.” Social Science Computer Review 40 (5): 1323–1339.Schoonvelde, Martijn, Anna Brosius, Gijs Schumacher, Bert N. Bakker. 2019. “Liberals Lecture, Conservatives Communicate: Analyzing Complexity Ideology 381,609 Political Speeches.” PLOS ONE 14 (2): e0208450. https://doi.org/10.1371/journal.pone.0208450Schoonvelde, Martijn, Anna Brosius, Gijs Schumacher, Bert N. Bakker. 2019. “Liberals Lecture, Conservatives Communicate: Analyzing Complexity Ideology 381,609 Political Speeches.” PLOS ONE 14 (2): e0208450. https://doi.org/10.1371/journal.pone.0208450","code":""},{"path":"readings.html","id":"week-5-scaling-techniques-unsupervised-learning-i","chapter":"Reading List","heading":"Week #5: Scaling Techniques (Unsupervised Learning I)","text":"Topics: unsupervised learning? scaling models, can tell us?Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 12–13.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 12–13.Slapin, Jonathan B., Sven-Oliver Proksch. 2008. “Scaling Model Estimating Time-Series Party Positions Texts.” American Journal Political Science 52 (3): 705–722.Slapin, Jonathan B., Sven-Oliver Proksch. 2008. “Scaling Model Estimating Time-Series Party Positions Texts.” American Journal Political Science 52 (3): 705–722.Denny, Matthew J., Arthur Spirling. 2018. “Text Preprocessing Unsupervised Learning: Matters, Misleads, .” Political Analysis 26 (2): 168–189.Denny, Matthew J., Arthur Spirling. 2018. “Text Preprocessing Unsupervised Learning: Matters, Misleads, .” Political Analysis 26 (2): 168–189.","code":""},{"path":"readings.html","id":"week-6-topic-modeling-and-clustering-unsupervised-learning-ii","chapter":"Reading List","heading":"Week #6: Topic Modeling and Clustering (Unsupervised Learning II)","text":"Topics: topic modeling, can tell us?Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 12–13.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 12–13.Roberts, Margaret E., et al. 2014. “Structural Topic Models Open-Ended Survey Responses.” American Journal Political Science 58 (4): 1064–1082.Roberts, Margaret E., et al. 2014. “Structural Topic Models Open-Ended Survey Responses.” American Journal Political Science 58 (4): 1064–1082.Motolinia, Lucia 2021. “Electoral Accountability Particularistic Legislation: Evidence Electoral Reform Mexico.” American Political Science Review 115 (1): 97–113.Motolinia, Lucia 2021. “Electoral Accountability Particularistic Legislation: Evidence Electoral Reform Mexico.” American Political Science Review 115 (1): 97–113.","code":""},{"path":"readings.html","id":"spring-reading-week","chapter":"Reading List","heading":"Spring reading week","text":"Topics: Enjoy break!","code":""},{"path":"readings.html","id":"week-7-a-primer-on-supervised-learning","chapter":"Reading List","heading":"Week #7: A Primer on Supervised Learning","text":"Topics: supervised learning? study framework training supervised models use . learn Support Vector Machine (SVM) Bidirectional Long-Short Term Memory (Bi-LSTM) models work.Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 17–20.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapters 17–20.Siegel, Alexandra ., et al. 2021. “Trumping Hate Twitter? Online Hate Speech 2016 US Election Campaign Aftermath.” Quarterly Journal Political Science 16 (1): 71–104.Siegel, Alexandra ., et al. 2021. “Trumping Hate Twitter? Online Hate Speech 2016 US Election Campaign Aftermath.” Quarterly Journal Political Science 16 (1): 71–104.Barberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, Jonathan Nagler. 2021. “Automated Text Classification News Articles: Practical Guide.” Political Analysis 29 (1): 19–42. https://doi.org/10.1017/pan.2020.8Barberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, Jonathan Nagler. 2021. “Automated Text Classification News Articles: Practical Guide.” Political Analysis 29 (1): 19–42. https://doi.org/10.1017/pan.2020.8Laver, Michael, Kenneth Benoit, John Garry. 2003. “Extracting Policy Positions Political Texts Using Words Data.” American Political Science Review 97 (2): 311–331.Laver, Michael, Kenneth Benoit, John Garry. 2003. “Extracting Policy Positions Political Texts Using Words Data.” American Political Science Review 97 (2): 311–331.Benoit, Kenneth, et al. 2016. “Crowd-Sourced Text Analysis: Reproducible Agile Production Political Data.” American Political Science Review 110 (2): 278–295.Benoit, Kenneth, et al. 2016. “Crowd-Sourced Text Analysis: Reproducible Agile Production Political Data.” American Political Science Review 110 (2): 278–295.","code":""},{"path":"readings.html","id":"week-8-introduction-to-deep-learning-and-word-embeddings","chapter":"Reading List","heading":"Week #8: Introduction to Deep Learning and Word Embeddings","text":"Topics: can capture meaning words? use deep learning models represent text.Readings:Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapter 8.Grimmer, Justin, Margaret E. Roberts, Brandon M. Stewart. 2022. Text Data: New Framework Machine Learning Social Sciences. Princeton, NJ: Princeton University Press. Chapter 8.Lin, Gechun, Christopher Lucas. 2023. “Introduction Neural Networks Social Sciences.” Oxford Handbook Engaged Methodological Pluralism Political Science, edited Janet M. Box-Steffensmeier, Dino P. Christenson, Valeria Sinclair-Chapman. Oxford: Oxford University Press. https://doi.org/10.1093/oxfordhb/9780192868282.013.79Lin, Gechun, Christopher Lucas. 2023. “Introduction Neural Networks Social Sciences.” Oxford Handbook Engaged Methodological Pluralism Political Science, edited Janet M. Box-Steffensmeier, Dino P. Christenson, Valeria Sinclair-Chapman. Oxford: Oxford University Press. https://doi.org/10.1093/oxfordhb/9780192868282.013.79Meyer, David. 2016. “Exactly word2vec Work?” https://davidmeyer.github.io/ml/how_does_word2vec_work.pdfMeyer, David. 2016. “Exactly word2vec Work?” https://davidmeyer.github.io/ml/how_does_word2vec_work.pdfAlammar, Jay. 2019. “Illustrated Word2vec.” https://jalammar.github.io/illustrated-word2vec/Alammar, Jay. 2019. “Illustrated Word2vec.” https://jalammar.github.io/illustrated-word2vec/Rodriguez, Pedro L., Arthur Spirling. 2022. “Word Embeddings: Works, Doesn’t, Tell Difference Applied Research.” Journal Politics 84 (1): 101–115. https://doi.org/10.1086/715162Rodriguez, Pedro L., Arthur Spirling. 2022. “Word Embeddings: Works, Doesn’t, Tell Difference Applied Research.” Journal Politics 84 (1): 101–115. https://doi.org/10.1086/715162Kozlowski, Austin C., Matt Taddy, James . Evans. 2019. “Geometry Culture: Analyzing Meanings Class Word Embeddings.” American Sociological Review 84 (5): 905–949.Kozlowski, Austin C., Matt Taddy, James . Evans. 2019. “Geometry Culture: Analyzing Meanings Class Word Embeddings.” American Sociological Review 84 (5): 905–949.","code":""},{"path":"readings.html","id":"week-9-the-transformers-architecture","chapter":"Reading List","heading":"Week #9: The Transformers Architecture","text":"Topics: learn Transformer architecture, attention, encoder-decoder framework.Readings:Alammar, Jay. 2018. “Illustrated Transformer.” https://jalammar.github.io/illustrated-transformer/Alammar, Jay. 2018. “Illustrated Transformer.” https://jalammar.github.io/illustrated-transformer/Vaswani, Ashish, et al. 2017. “Attention Need.” Advances Neural Information Processing Systems 30.Vaswani, Ashish, et al. 2017. “Attention Need.” Advances Neural Information Processing Systems 30.Devlin, Jacob, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2018. “BERT: Pre-Training Deep Bidirectional Transformers Language Understanding.” arXiv preprint arXiv:1810.04805.Devlin, Jacob, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2018. “BERT: Pre-Training Deep Bidirectional Transformers Language Understanding.” arXiv preprint arXiv:1810.04805.Timoneda, Joan C., Sebastián Vallejo Vera. 2025. “BERT, RoBERTa, DeBERTa? Comparing Performance Across Transformer Models Political Science Text.” Journal Politics 87 (1): 347–364. https://doi.org/10.1086/730737Timoneda, Joan C., Sebastián Vallejo Vera. 2025. “BERT, RoBERTa, DeBERTa? Comparing Performance Across Transformer Models Political Science Text.” Journal Politics 87 (1): 347–364. https://doi.org/10.1086/730737","code":""},{"path":"readings.html","id":"week-10-encoder-only-llms","chapter":"Reading List","heading":"Week #10: Encoder-Only LLMs","text":"Topics: take deep dive encoder-LLMs can .Readings:Taylor, Wilson L. 1953. “ ‘Cloze Procedure’: New Tool Measuring Readability.” Journalism Quarterly 30 (4): 415–433.Taylor, Wilson L. 1953. “ ‘Cloze Procedure’: New Tool Measuring Readability.” Journalism Quarterly 30 (4): 415–433.Dávila Gordillo, Diana, Joan C. Timoneda, Sebastián Vallejo Vera. Forthcoming. “Machines See Color: Guideline Classify Different Forms Racist Discourse Large Corpora.” Sociological Methods & Research. arXiv:2401.09333.Dávila Gordillo, Diana, Joan C. Timoneda, Sebastián Vallejo Vera. Forthcoming. “Machines See Color: Guideline Classify Different Forms Racist Discourse Large Corpora.” Sociological Methods & Research. arXiv:2401.09333.","code":""},{"path":"readings.html","id":"week-11-decoder-only-llms","chapter":"Reading List","heading":"Week #11: Decoder-Only LLMs","text":"Topics: Decoder-LLMs, also known generative LLMs, rage right now. study work, can , limitations , can use work broadly.Readings:Lee, Kyuwon, Simone Paci, Jeongmin Park, Hye Young , Sylvan Zheng. 2025. “Applications GPT Political Science Research: Extracting Information Unstructured Text.” PS: Political Science & Politics 58 (4): 1–11. https://doi.org/10.1017/S1049096525000046Lee, Kyuwon, Simone Paci, Jeongmin Park, Hye Young , Sylvan Zheng. 2025. “Applications GPT Political Science Research: Extracting Information Unstructured Text.” PS: Political Science & Politics 58 (4): 1–11. https://doi.org/10.1017/S1049096525000046Gilardi, Fabrizio, Meysam Alizadeh, Maël Kubli. 2023. “ChatGPT Outperforms Crowd Workers Text-Annotation Tasks.” Proceedings National Academy Sciences 120 (30): e2305016120. https://doi.org/10.1073/pnas.2305016120Gilardi, Fabrizio, Meysam Alizadeh, Maël Kubli. 2023. “ChatGPT Outperforms Crowd Workers Text-Annotation Tasks.” Proceedings National Academy Sciences 120 (30): e2305016120. https://doi.org/10.1073/pnas.2305016120Heseltine, Michael, Bernhard Clemm von Hohenberg. 2024. “Large Language Models Substitute Human Experts Annotating Political Text.” Research & Politics 11 (1): 20531680241236239. https://doi.org/10.1177/20531680241236239Heseltine, Michael, Bernhard Clemm von Hohenberg. 2024. “Large Language Models Substitute Human Experts Annotating Political Text.” Research & Politics 11 (1): 20531680241236239. https://doi.org/10.1177/20531680241236239Vallejo Vera, Sebastián, Hunter Driggers. 2025. “LLMs Annotators: Effect Party Cues Labelling Decisions Large Language Models.” Humanities Social Sciences Communications 12: Article 1530. https://doi.org/10.1057/s41599-025-05834-4Vallejo Vera, Sebastián, Hunter Driggers. 2025. “LLMs Annotators: Effect Party Cues Labelling Decisions Large Language Models.” Humanities Social Sciences Communications 12: Article 1530. https://doi.org/10.1057/s41599-025-05834-4Walker, Christina P., Joan C. Timoneda. 2025. “ChatGPT Conservative Liberal? Novel Approach Assess Ideological Stances Biases Generative LLMs.” Political Science Research Methods 1–15. https://doi.org/10.1017/psrm.2025.10057Walker, Christina P., Joan C. Timoneda. 2025. “ChatGPT Conservative Liberal? Novel Approach Assess Ideological Stances Biases Generative LLMs.” Political Science Research Methods 1–15. https://doi.org/10.1017/psrm.2025.10057","code":""},{"path":"assignments.html","id":"assignments","chapter":"Assignments","heading":"Assignments","text":"three worksheets (Exercise #N) walk implementation different text analysis techniques. end worksheet, find set questions. optional (even though strongly suggest answer ), graded assignments (shown ). partner someone else class go together. ? Christopher Barrie says: “called pair programming ’s reason . Firstly, coding can isolating difficult thing—’s good bring friend along ride! Secondly, ’s something don’t know, maybe partner . saves time. Thirdly, partner can check code write , vice versa. , means working together produce check something go along.”assignment (optional graded) due, pick pair (individual, prefer work alone) random answer questions walk us code. punitive exercise, rather space collaborative learning. often , obstacles encountered one person also encountered many others. Furthermore, many ways arrive solution, exposed different approaches beneficial everyone. matters try , eventually, learn. (goes turn assignment.)total, grade assignments worth 30% grade.","code":""},{"path":"assignments.html","id":"instruction-for-submission","chapter":"Assignments","heading":"Instruction for Submission","text":"assignments graded accuracy quality programming style. following elements looking grading:code must run.code must run.Solutions readable:\nCode thoroughly commented (able understand code’s purpose reading comments).\nCoding solutions broken individual code chunks Jupyter R Markdown notebooks, clumped together one large code chunk.\nstudent-defined function include docstring explaining function , input argument, function returns (required define functions).\nSolutions readable:Code thoroughly commented (able understand code’s purpose reading comments).Code thoroughly commented (able understand code’s purpose reading comments).Coding solutions broken individual code chunks Jupyter R Markdown notebooks, clumped together one large code chunk.Coding solutions broken individual code chunks Jupyter R Markdown notebooks, clumped together one large code chunk.student-defined function include docstring explaining function , input argument, function returns (required define functions).student-defined function include docstring explaining function , input argument, function returns (required define functions).Commentary, responses, /solutions written Markdown explain outputs sufficiently.Commentary, responses, /solutions written Markdown explain outputs sufficiently.","code":""},{"path":"assignments.html","id":"assignments-and-due-date","chapter":"Assignments","heading":"Assignments and Due Date","text":"assignments worth 10 points.","code":""},{"path":"assignments.html","id":"assignments-1---due-date-eod-friday-week-4","chapter":"Assignments","heading":"Assignments 1 - Due Date: EOD Friday Week 4","text":"assignment must completed covering Week 3: Dictionary-Based Techniques.Replicate results left-column Figure 3 Ventura et al. (2021).Look keywords context Biden ventura_etal_df dataset, compare results data, pre-processed (.e., lower-case, remove stopwords, etc.). version provides information context Biden appears comments?Use different collocation approach ventura_etal_df dataset, pre-process data (.e., lower-case, remove stopwords, etc.). approach (pre-processed pre-processed) provides better picture corpus collocations found?Compare positive sentiment comments mentioning trump comments mentioning biden using bing afinn. Note afinn gives numeric value, need choose threshold determine positive sentiment.Using bing, compare sentiment comments mentioning trump comments mentioning biden using different metrics (e.g., Young Soroka 2012, Martins Baumard 2020, Ventura et al. 2021).Create domain-specific dictionary apply ventura_etal_df dataset. Show limitations dictionary (e.g., false positives), comment much problem wanted conduct analysis corpus.","code":""},{"path":"assignments.html","id":"assignments-2---due-date-eod-friday-week-6","chapter":"Assignments","heading":"Assignments 2 - Due Date: EOD Friday Week 6","text":"assignment must completed covering Week 5: Scaling Techniques (Unsupervised Learning ).hard time scaling text, looked possible problems. possible solutions want position U.S. presidents ideological scale using text?Use data/candidate-tweets.csv data run STM. Decide covariates going . Decide whether use data sample data. Decide whether going aggregate split text way (.e., decide unit analysis). Decide number topics look (try one option). can tell topics tweeted 2015 U.S. primary candidates?Choose three topics. Can place candidates ideological scale within topic (determine \\(\\theta\\) threshold can say tweet mostly topic)? make sense? ?","code":""},{"path":"assignments.html","id":"assignments-3---due-date-eod-friday-week-11","chapter":"Assignments","heading":"Assignments 3 - Due Date: EOD Friday Week 11","text":"first part assignment must completed covering Week 7: Primer Supervised Learning. second part assignment must completed covering Week 10: Encoder-LLMs.Part :Think dataset (corpus) classification task. Ideally, corpus classification task can used final paper. However, ’s OK assignment (still need corpus). can choose task except sentiment classification.Decide number categories predicting.Decide number observations code per category.Create draft codebook guide coders (hypothetically) label training set.Label sample data (N=200); decide sample data explain decision. classmate label sample (can find coder pairing ). Estimate inter-coder reliability evaluate results.difficult easy task? problems run ? change codebook improve ? lessons learn exercise?Note: use codebook training set Part II. can also basis final paper.Part II:validated training set labels produced LLMs, expand training set (least) 150 observations per category. can use LLMs label, still need manually validate labels produced.Using (validated) training set, fine-tune encoder-LLM choice.Report performance statistics fine-tuned model (e.g., accuracy, F1, recall, etc.), well confusion matrix.Use trained model predict labels target corpus.Describe results obtained, whether match expectations data.","code":""},{"path":"replication.html","id":"replication","chapter":"Replication Exercise","heading":"Replication Exercise","text":"Replication exercises adapted Gary King’s work . core, replication process re-running study using authors’ original data code, checking whether can recover published results (, relevant, bringing new data conversation). Replication matters scientific knowledge becomes credible, cumulative, actually useful.purposes, replication primarily educational tool. common saying science really learn method use work. Since time write three full papers one semester, instead take advantage published research openly available datasets code, treat replication structured, step--step way learning .replication exercise worth 30% grade.","code":""},{"path":"replication.html","id":"key-date","chapter":"Replication Exercise","heading":"Key Date","text":"Week 3: Choose paper replicate.Week 12: Present replication.EOD Friday Week 12: Upload paper code repository.","code":""},{"path":"replication.html","id":"step-by-step-for-replication","chapter":"Replication Exercise","heading":"Step-by-Step for Replication","text":"exercise look like:Step 1: Choosing paper replicate\nend Week 3, select one article syllabus replicate.Step 1: Choosing paper replicate\nend Week 3, select one article syllabus replicate.Step 2: Acquiring data code\nMany journals now open-data open-materials expectations, means authors often make replication files available (commonly GitHub Harvard Dataverse). first task locate data code associated chosen article.\narticle replication materials publicly available, :\nPolitely contact authors request replication materials; \nreceive response within reasonable amount time, select different article.\nStep 2: Acquiring data code\nMany journals now open-data open-materials expectations, means authors often make replication files available (commonly GitHub Harvard Dataverse). first task locate data code associated chosen article.article replication materials publicly available, :Politely contact authors request replication materials; andIf receive response within reasonable amount time, select different article.Step 3: Presentation\nWeek 12, present replication efforts. presentation include:\nIntroduction: short summary article main claims;\nMethods: description data empirical strategy used article;\nResults: able replicate (tables, figures, key quantities);\nDifferences: differences results authors’ results;\nReplication autopsy: worked, , things got stuck (often informative part);\nExtension: writing paper today, differently? innovate?\nStep 3: Presentation\nWeek 12, present replication efforts. presentation include:Introduction: short summary article main claims;Methods: description data empirical strategy used article;Results: able replicate (tables, figures, key quantities);Differences: differences results authors’ results;Replication autopsy: worked, , things got stuck (often informative part);Extension: writing paper today, differently? innovate?Step 4: Replication repository report\nend--day Friday replication week, share replication materials classmates. replication package include:\nGitHub repository clear, well-documented README. (model available .)\npresentation PDF;\ncode used replication reproducible notebook (R Markdown Jupyter);\nshort written report (maximum 5 pages; fewer totally fine) summarizing replication process, emphasis four parts presentation: Results, Differences, Replication autopsy, Extension.\nStep 4: Replication repository report\nend--day Friday replication week, share replication materials classmates. replication package include:GitHub repository clear, well-documented README. (model available .)presentation PDF;code used replication reproducible notebook (R Markdown Jupyter);short written report (maximum 5 pages; fewer totally fine) summarizing replication process, emphasis four parts presentation: Results, Differences, Replication autopsy, Extension.addition following requirements , replication exercises also graded accuracy quality programming style. instance, looking :code must run.Solutions readable:\nCode thoroughly commented (able understand code’s purpose reading comments).\nCoding solutions broken individual code chunks Jupyter R Markdown notebooks, clumped together one large code chunk.\nstudent-defined function include docstring explaining function , input argument, function returns (using functions).\nCode thoroughly commented (able understand code’s purpose reading comments).Coding solutions broken individual code chunks Jupyter R Markdown notebooks, clumped together one large code chunk.student-defined function include docstring explaining function , input argument, function returns (using functions).Commentary, responses, /solutions written Markdown explain outputs sufficiently.","code":""},{"path":"final_paper.html","id":"final_paper","chapter":"Final Paper","heading":"Final Paper","text":"paper complete description project’s analysis results. 4,000 words max (single-spaced; 12pt font). front page references count toward word limit. Week 13, student present final project class. final paper due April 16. final paper worth 40% grade.","code":""},{"path":"final_paper.html","id":"paper-outline","chapter":"Final Paper","heading":"Paper Outline","text":", outline aim cover section. Note paper read cohesive report (set disconnected answers prompts):Introduction\nSummarize motivation briefly discuss prior work related question.\nState research question.\nProvide roadmap report.\nSummarize motivation briefly discuss prior work related question.State research question.Provide roadmap report.Data Methods\ndata come ?\nunit observation?\nvariables interest?\nsteps take wrangle data?\ndata come ?unit observation?variables interest?steps take wrangle data?Analysis\nDescribe methods/tools explored project implemented .\nDescribe methods/tools explored project implemented .Results\nProvide detailed summary results.\nPresent results clearly concisely.\nUse visualizations instead tables whenever possible.\nProvide detailed summary results.Present results clearly concisely.Use visualizations instead tables whenever possible.Discussion\nRe-introduce main results.\nState contributions.\nExplain want take project next.\nRe-introduce main results.State contributions.Explain want take project next.Important note (literature reviews).\nstandalone literature review section report. absolutely use literature motivate work situate question, need full section tries summarize entire field. want examples style, read papers general-interest journals: many excellent articles long, self-contained literature review sections.","code":""},{"path":"final_paper.html","id":"submission-of-the-final-project","chapter":"Final Paper","heading":"Submission of the final project","text":"end product GitHub repository contains:raw source data used project. data large GitHub, talk find solution.proposal.README documents repository. file, README clearly describe:\nInputs file (e.g., raw data; credentials needed access API),\nfile (major transformations), \nOutputs produced file (e.g., cleaned dataset; figure).\nInputs file (e.g., raw data; credentials needed access API),file (major transformations), andOutputs produced file (e.g., cleaned dataset; figure).code files transform raw data form usable answer question.final 4,000-word essay.course, commits deadline considered assessment.","code":""},{"path":"final_paper.html","id":"templates-for-writing","chapter":"Final Paper","heading":"Templates for writing","text":"templates can use writing report. experience, writing data-heavy social science essays LaTeX productivity boost long run: makes formatting predictable, citations painless, reproducibility easier.want experiment LaTeX via Overleaf, can start PNAS template (just remember switch one-column format):PNAS templateYou can also use Journal Quantitative Description templates (Word LaTeX):JQD WordJQD LaTeXFinally, can use templates Quarto write entire project using Quarto (Markdown) files. perfectly valid workflow course.","code":""},{"path":"week1.html","id":"week1","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1 Week 1: A Primer on Using Text as Data","text":"","code":""},{"path":"week1.html","id":"slides","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"Slides","text":"1 Introduction CTA (link2 Computational Text Analysis? (link","code":""},{"path":"week1.html","id":"setup","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.1 Setup","text":"first example, replicate (extend) Mendenhall’s (1887) Mendenhall’s (1901) studies word-length distribution.\nFigure 1.1: Mendenhall (1987) - Characteristic Curves Composition.\nFirst load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(wesanderson) # to prettify\nlibrary(gutenbergr) # to get some books\nlibrary(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week1.html","id":"get-data","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.2 Get Data","text":"Mendenhall (1887) argued “every writer makes use vocabulary peculiar , character materially change year year productive [years],” one characteristics word length. Mendenhall (1901) takes suggests , given assumption, Shakespeare Bacon person1.Let’s get corpus–collection documents–can analyze. can search Gutenberg repository create corpus selected works.","code":"\n# Inspect Project Gutenberg metadata to find all texts authored by Oscar Wilde.\n# This returns a table of matching Gutenberg entries (including IDs you can use to download texts).\ngutenberg_metadata %>%\n  filter(author == \"Wilde, Oscar\")## # A tibble: 67 × 8\n##    gutenberg_id title                   author\n##           <int> <chr>                   <chr> \n##  1          174 The Picture of Dorian … Wilde…\n##  2          301 The Ballad of Reading … Wilde…\n##  3          773 Lord Arthur Savile's C… Wilde…\n##  4          774 Essays and Lectures     Wilde…\n##  5          790 Lady Windermere's Fan   Wilde…\n##  6          844 The Importance of Bein… Wilde…\n##  7          854 A Woman of No Importan… Wilde…\n##  8          873 A House of Pomegranates Wilde…\n##  9          875 The Duchess of Padua    Wilde…\n## 10          885 An Ideal Husband        Wilde…\n## # ℹ 57 more rows\n## # ℹ 5 more variables:\n## #   gutenberg_author_id <int>,\n## #   language <fct>,\n## #   gutenberg_bookshelf <chr>, rights <fct>,\n## #   has_text <lgl>"},{"path":"week1.html","id":"word-length-in-wildes-corpus","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.3 Word Length in Wilde’s Corpus","text":"’s lot Wilde! Let’s focus four plays: “Importance Earnest”, “Woman Importance”, “Lady Windermere’s Fan”, “Ideal Husband”. can download plays using Gutenberg ID numbers:case, unit analysis something like line. interested word–also known token–length within play. clean unwanted text–text add noise analysis–count number words.Now, can change unit analysis token:’s lot words! now create column word length, count number words length (play!).Let’s see distribution word length play:problem. ?solution (proposed Mendenhall):look , Mendenhall onto something: author may signature terms word-length distribution. Wilde, obvious change across time (play published different year). happens compare Wilde’s signature Shakespeare’s? Let’s choose four plays (random) Shakespeare: Midsummer Night’s Dream, Merchant Venice, Much Ado Nothing, Tempest.","code":"\n# Download four Oscar Wilde plays from Project Gutenberg using their Gutenberg IDs.\n# The IDs correspond to specific texts in the Gutenberg catalog.\n# `meta_fields` appends the requested metadata (here: title and author) to each row of text.\nwilde <- gutenberg_download(\n  c(790, 844, 854, 885),\n  meta_fields = c(\"title\", \"author\")\n)\n\n# Quick inspection: print rows 51–75 (often useful for seeing the structure of the raw text)\n# and show up to 25 rows in the console output.\nprint(n = 25, wilde[c(51:75), ])## # A tibble: 25 × 4\n##    gutenberg_id text              title author\n##           <int> <chr>             <chr> <chr> \n##  1          790 \"\"                Lady… Wilde…\n##  2          790 \"\"                Lady… Wilde…\n##  3          790 \"THE PERSONS OF … Lady… Wilde…\n##  4          790 \"\"                Lady… Wilde…\n##  5          790 \"\"                Lady… Wilde…\n##  6          790 \"Lord Windermere\" Lady… Wilde…\n##  7          790 \"\"                Lady… Wilde…\n##  8          790 \"Lord Darlington\" Lady… Wilde…\n##  9          790 \"\"                Lady… Wilde…\n## 10          790 \"Lord Augustus L… Lady… Wilde…\n## 11          790 \"\"                Lady… Wilde…\n## 12          790 \"Mr. Dumby\"       Lady… Wilde…\n## 13          790 \"\"                Lady… Wilde…\n## 14          790 \"Mr. Cecil Graha… Lady… Wilde…\n## 15          790 \"\"                Lady… Wilde…\n## 16          790 \"Mr. Hopper\"      Lady… Wilde…\n## 17          790 \"\"                Lady… Wilde…\n## 18          790 \"Parker, Butler\"  Lady… Wilde…\n## 19          790 \"\"                Lady… Wilde…\n## 20          790 \"               … Lady… Wilde…\n## 21          790 \"\"                Lady… Wilde…\n## 22          790 \"Lady Windermere\" Lady… Wilde…\n## 23          790 \"\"                Lady… Wilde…\n## 24          790 \"The Duchess of … Lady… Wilde…\n## 25          790 \"\"                Lady… Wilde…\nwilde <- wilde %>%\n  # Standardize the title for \"The Importance of Being Earnest\"\n  # (Gutenberg titles can vary slightly across editions/records).\n  mutate(\n    title = ifelse(\n      str_detect(title, \"Importance of Being\"),\n      \"The Importance of Being Earnest\",\n      title\n    )\n  ) %>%\n  # Remove empty lines (blank rows add noise and can affect tokenization/counts).\n  filter(text != \"\") %>%\n  # Remove speaker labels typical of plays (often written in ALL CAPS).\n  # This keeps primarily spoken text rather than character-name headers.\n  filter(str_detect(text, \"[A-Z]{3,}\") == FALSE)\n\n# Inspect a slice of the cleaned text to confirm the filters behaved as expected.\nprint(n = 25, wilde[c(51:75), ])## # A tibble: 25 × 4\n##    gutenberg_id text              title author\n##           <int> <chr>             <chr> <chr> \n##  1          790 \"tea-table L._  … Lady… Wilde…\n##  2          790 \"home to any one… Lady… Wilde…\n##  3          790 \"               … Lady… Wilde…\n##  4          790 \"he’s come.\"      Lady… Wilde…\n##  5          790 \"hands with you.… Lady… Wilde…\n##  6          790 \"lovely?  They c… Lady… Wilde…\n##  7          790 \"table_.]  And w… Lady… Wilde…\n##  8          790 \"everything.  I … Lady… Wilde…\n##  9          790 \"present to me. … Lady… Wilde…\n## 10          790 \"life, isn’t it?… Lady… Wilde…\n## 11          790 \"down.  [_Still … Lady… Wilde…\n## 12          790 \"birthday, Lady … Lady… Wilde…\n## 13          790 \"front of your h… Lady… Wilde…\n## 14          790 \"you.\"            Lady… Wilde…\n## 15          790 \"               … Lady… Wilde…\n## 16          790 \"Foreign Office.… Lady… Wilde…\n## 17          790 \"with her pocket… Lady… Wilde…\n## 18          790 \"Won’t you come … Lady… Wilde…\n## 19          790 \"miserable, Lady… Lady… Wilde…\n## 20          790 \"table L._]\"      Lady… Wilde…\n## 21          790 \"whole evening.\"  Lady… Wilde…\n## 22          790 \"that the only p… Lady… Wilde…\n## 23          790 \"things we _can_… Lady… Wilde…\n## 24          790 \"You mustn’t lau… Lady… Wilde…\n## 25          790 \"don’t see why a… Lady… Wilde…\nwilde_words <- wilde %>%\n  # Tokenize: split the `text` column into one word per row.\n  # The output column is named `word`; punctuation is removed and words are lowercased by default.\n  unnest_tokens(word, text) %>%\n  # Remove underscores (some Gutenberg texts include formatting artifacts like \"_\" that add noise).\n  mutate(word = str_remove_all(word, \"\\\\_\"))\n\n# View the tokenized dataset (one row per token, with title/author carried along).\nwilde_words## # A tibble: 60,477 × 4\n##    gutenberg_id title             author word \n##           <int> <chr>             <chr>  <chr>\n##  1          790 Lady Windermere'… Wilde… by   \n##  2          790 Lady Windermere'… Wilde… sixt…\n##  3          790 Lady Windermere'… Wilde… edit…\n##  4          790 Lady Windermere'… Wilde… first\n##  5          790 Lady Windermere'… Wilde… publ…\n##  6          790 Lady Windermere'… Wilde… 1893 \n##  7          790 Lady Windermere'… Wilde… first\n##  8          790 Lady Windermere'… Wilde… issu…\n##  9          790 Lady Windermere'… Wilde… by   \n## 10          790 Lady Windermere'… Wilde… meth…\n## # ℹ 60,467 more rows\nwilde_words_ct <- wilde_words %>%\n  # Compute the length (number of characters) of each token\n  mutate(word_length = str_length(word)) %>%\n  # Group by play title and word length to build the word-length distribution\n  group_by(word_length, title) %>%\n  # Count how many tokens fall into each (word_length, title) bin\n  # (n() returns the group size; mutate repeats it on every row in the group)\n  mutate(total_word_length = n()) %>%\n  # Keep a single row per (word_length, title) combination\n  distinct(word_length, title, .keep_all = TRUE) %>%\n  # Keep only the variables needed for plotting/inspection\n  dplyr::select(word_length, title, author, total_word_length)\nwilde_words_ct %>%\n  # Plot the word-length distribution for each play\n  ggplot(aes(x = word_length, y = total_word_length, color = title)) +\n  # Points show observed counts at each word length\n  geom_point(alpha = 0.8) +\n  # Lines connect points to make the distribution shape easier to see\n  geom_line(alpha = 0.8) +\n  # Use a Wes Anderson palette for play colors\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  # Clean, minimal theme\n  theme_minimal() +\n  # Place legend on the right for readability\n  theme(legend.position = \"right\") +\n  # Axis labels (x = word length in characters; y = number of tokens of that length)\n  labs(x = \"Length\", y = \"Total Number of Words\", color = \"\")\nwilde_words %>%\n  # Work within each play separately\n  group_by(title) %>%\n  # Take an equal-sized random sample of tokens from each play\n  # (this makes the resulting distributions comparable across plays)\n  slice_sample(n = 10000) %>%\n  # Compute word length for each token, and the median word length within each play (on the sampled data)\n  mutate(\n    word_length = str_length(word),\n    median_word_length = median(word_length)\n  ) %>%\n  # Count how many sampled tokens fall into each word-length bin, within each play\n  group_by(word_length, title) %>%\n  mutate(total_word_length = n()) %>%\n  # Keep one row per (word_length, title) combination for plotting\n  distinct(word_length, title, .keep_all = TRUE) %>%\n  # Keep relevant columns (median_word_length is repeated but useful for plotting the median line)\n  dplyr::select(word_length, title, author, total_word_length, median_word_length) %>%\n  # Plot the sampled word-length distributions\n  ggplot(aes(x = word_length, y = total_word_length, color = title)) +\n  geom_point(alpha = 0.8) +\n  geom_line(alpha = 0.8) +\n  # Add a vertical line at each play's median word length\n  geom_vline(aes(xintercept = median_word_length, color = title, linetype = title)) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(\n    x = \"Length\",\n    y = \"Total Number of Words\",\n    color = \"\",\n    linetype = \"\",\n    caption = \"Note: Line type shows median word length.\"\n  )## slice_sample (grouped): removed 20,477 rows\n## (34%), 40,000 rows remaining (removed 0\n## groups, 4 groups remaining)"},{"path":"week1.html","id":"comparing-shakespeare-and-wilde","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.4 Comparing Shakespeare and Wilde","text":"text cleaner Wilde’s corpus, leave . Also, harder systematically remove name person speaking. problem? ? ?can put together corpora see differences distributions word length.differences? can conclude evidence? limitations approach? alternative approaches study Mendenhall getting ?","code":"\nshakes <- gutenberg_download(c(1520,2242,2243,2235),\n                             meta_fields = c(\"title\",\"author\"))\nprint(n=25,shakes[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text              title author\n##           <int> <chr>             <chr> <chr> \n##  1         1520 \"[Enter Leonato,… Much… Shake…\n##  2         1520 \"\"                Much… Shake…\n##  3         1520 \"Leon.\"           Much… Shake…\n##  4         1520 \"I learn in this… Much… Shake…\n##  5         1520 \"night to Messin… Much… Shake…\n##  6         1520 \"\"                Much… Shake…\n##  7         1520 \"Mess.\"           Much… Shake…\n##  8         1520 \"He is very near… Much… Shake…\n##  9         1520 \"left him.\"       Much… Shake…\n## 10         1520 \"\"                Much… Shake…\n## 11         1520 \"Leon.\"           Much… Shake…\n## 12         1520 \"How many gentle… Much… Shake…\n## 13         1520 \"\"                Much… Shake…\n## 14         1520 \"Mess.\"           Much… Shake…\n## 15         1520 \"But few of any … Much… Shake…\n## 16         1520 \"\"                Much… Shake…\n## 17         1520 \"Leon.\"           Much… Shake…\n## 18         1520 \"A victory is tw… Much… Shake…\n## 19         1520 \"numbers.  I fin… Much… Shake…\n## 20         1520 \"a young Florent… Much… Shake…\n## 21         1520 \"\"                Much… Shake…\n## 22         1520 \"Mess.\"           Much… Shake…\n## 23         1520 \"Much deserved o… Much… Shake…\n## 24         1520 \"He hath borne h… Much… Shake…\n## 25         1520 \"in the figure o… Much… Shake…\nshakes_words <- shakes %>%\n  # Filter out all empty rows\n  filter(text != \"\") %>%\n  # This is a play. The name of each character before they speak \n  filter(str_detect(text,\"[A-Z]{3,}\")==FALSE) %>%\n  # take the column text and divide it by words\n  unnest_tokens(word, text) \n\n# Bind both word dfs\nwords <- rbind.data.frame(shakes_words,wilde_words)\n\n# Count words etc.\nwords %>%\n  group_by(title,author) %>%\n  slice_sample(n=10000) %>%\n  mutate(word_length = str_length(word),\n         median_word_length = median(word_length)) %>%\n  group_by(word_length,title,author) %>%\n  mutate(total_word_length = n()) %>%\n  distinct(word_length,title,.keep_all=T) %>%\n  dplyr::select(word_length,title,author,total_word_length,median_word_length) %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=author,group=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  # facet_wrap(~author, ncol = 2)+\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\", linetype = \"\",\n       caption = \"Note: Median word length is 4 for both authors.\")## slice_sample (grouped): removed 53,665 rows\n## (43%), 70,000 rows remaining (removed 0\n## groups, 7 groups remaining)"},{"path":"week1.html","id":"exercise-optional","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.5 Exercise (Optional)","text":"Extend current analysis authors works author.better ways compare distribution word length? changes across time? differences different types works (e.g., fiction vs. non-fiction, prose vs. poetry)?","code":""},{"path":"week1.html","id":"final-words","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.6 Final Words","text":"often case, won’t able cover every single feature different packages offer, show every object create, explore everything can . advice go home explore code detail. Try applying different corpus come next class questions (just show able ).","code":""},{"path":"week2.html","id":"week2","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2 Week 2: Tokenization and Word Frequency","text":"","code":""},{"path":"week2.html","id":"slides-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"Slides","text":"3 Tokenization Word Frequency (link Perusall)","code":""},{"path":"week2.html","id":"setup-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(wesanderson) # to prettify\nlibrary(readxl) # to read excel\nlibrary(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week2.html","id":"get-data-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.2 Get Data:","text":"example, using small corpus song lyrics.Ok, different artists, different genres years…lyrics following form:","code":"\nsample_lyrics <- read_excel(\"data/lyrics_sample.xlsx\")\nhead(sample_lyrics)## # A tibble: 6 × 5\n##   artist              album  year song  lyrics\n##   <chr>               <chr> <dbl> <chr> <chr> \n## 1 Rage Against the M… Evil…  1996 Bull… \"Come…\n## 2 Rage Against the M… Rage…  1992 Kill… \"Kill…\n## 3 Rage Against the M… Rene…  2000 Rene… \"No m…\n## 4 Rage Against the M… The …  1999 Slee… \"Yeah…\n## 5 Rage Against the M… The …  1999 Guer… \"Tran…\n## 6 Rage Against the M… The …  1999 Test… \"Uh!\\…## \n##      Megan Thee Stallion \n##                        5 \n## Rage Against the Machine \n##                        6 \n##         System of a Down \n##                        5 \n##             Taylor Swift \n##                        5## [1] \"Yeah\\r\\n\\r\\nThe world is my expense\\r\\nIt’s the cost of my desire\\r\\nJesus blessed me with its future\\r\\nAnd I protect it with fire\\r\\n\\r\\nSo raise your fists and march around\\r\\nJust don’t take what you need\\r\\nI’ll jail and bury those committed\\r\\nAnd smother the rest in greed\\r\\n\\r\\nCrawl with me into tomorrow\\r\\nOr I’ll drag you to your grave\\r\\nI’m deep inside your children\\r\\nThey’ll betray you in my name\\r\\n\\r\\nHey, hey, sleep now in the fire\\r\\nHey, hey, sleep now in the fire\\r\\n\\r\\nThe lie is my expense\\r\\nThe scope of my desire\\r\\nThe party blessed me with its future\\r\\nAnd I protect it with fire\\r\\n\\r\\nI am the Niña, the Pinta, the Santa María\\r\\nThe noose and the rapist, the fields overseer\\r\\nThe Agents of Orange, the Priests of Hiroshima\\r\\nThe cost of my desire, sleep now in the fire\\r\\n\\r\\nHey, hey, sleep now in the fire\\r\\nHey, hey, sleep now in the fire\\r\\n\\r\\nFor it’s the end of history\\r\\nIt’s caged and frozen still\\r\\nThere is no other pill to take\\r\\nSo swallow the one that made you ill\\r\\n\\r\\nThe Niña, the Pinta, the Santa María\\r\\nThe noose and the rapist, the fields overseer\\r\\nThe Agents of Orange, the Priests of Hiroshima\\r\\nThe cost of my desire to sleep now in the fire\\r\\n\\r\\nYeah\\r\\n\\r\\nSleep now in the fire\\r\\nSleep now in the fire\\r\\nSleep now in the fire\\r\\nSleep now in the fire\""},{"path":"week2.html","id":"cleaning-the-text","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.3 Cleaning the Text","text":"Much like music, text comes different forms qualities. Regex workshop, might remember special characters can signal, example, new line (\\n) carriage return (\\r). example, can remove them2. working text, always check state documents loaded program choice.","code":"\nsample_lyrics <- sample_lyrics %>%\n  # Replace newline characters (\\n) with a period.\n  # Note: \"\\\\n\" matches the literal newline escape sequence in the string.\n  mutate(\n    lyrics_clean = str_replace_all(lyrics, \"\\\\n\", \"\\\\.\"),\n    # Replace carriage returns (\\r) with a period as well.\n    lyrics_clean = str_replace_all(lyrics_clean, \"\\\\r\", \"\\\\.\")\n  ) %>%\n  # Drop the original lyrics column to avoid keeping both raw and cleaned versions\n  dplyr::select(-lyrics)\n\n# Inspect the 4th cleaned lyric to confirm the replacements worked as intended\nsample_lyrics$lyrics_clean[4]## [1] \"Yeah....The world is my expense..It’s the cost of my desire..Jesus blessed me with its future..And I protect it with fire....So raise your fists and march around..Just don’t take what you need..I’ll jail and bury those committed..And smother the rest in greed....Crawl with me into tomorrow..Or I’ll drag you to your grave..I’m deep inside your children..They’ll betray you in my name....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....The lie is my expense..The scope of my desire..The party blessed me with its future..And I protect it with fire....I am the Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire, sleep now in the fire....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....For it’s the end of history..It’s caged and frozen still..There is no other pill to take..So swallow the one that made you ill....The Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire to sleep now in the fire....Yeah....Sleep now in the fire..Sleep now in the fire..Sleep now in the fire..Sleep now in the fire\""},{"path":"week2.html","id":"tokenization","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.4 Tokenization","text":"goal create document-feature matrix, later extract information word frequency. , start creating corpus object using quanteda package.Looks good. Now can tokenize corpus (reduce complexity). One benefit creating corpus object first preserves metadata document tokenize. come handy later.got rid punctuation. Now let’s remove stop words, high- low-frequency words, stem remaining tokens. cheating, though: already know words high- low-frequency inspected dfm (see next code chunk).can compare stemmed output non-stemmed output. “future” become “futur”? stemming assumes , purposes, “future” “futuristic” treated underlying root. Whether assumption appropriate depends research question. Finally, can create document-feature matrix (dfm).Note create dfm object, tokens become lowercase. Now can check 15 frequent tokens.tell us much, used previous code check low-information tokens might want remove analysis. can also see many tokens appear :interesting text analysis count words time/space. case, ‘space’ can artist.Interesting. lot overlap (apart one token shared Megan Thee Stallion Rage Machine). However, great measure importance word relative widely appears across documents (.e., normalize document prevalence). Enter TF-IDF: “term frequency–inverse document frequency.” TF-IDF weighting -weights relatively rare words–words appear many documents. combining term frequency inverse document frequency, can identify words especially characteristic given document within collection.building dictionary, example, might want include words high TF-IDF values. Another way think TF-IDF terms predictive power. Words common documents little predictive power receive TF-IDF value close 0. Words appear relatively small number documents tend greater predictive power receive higher TF-IDF values. rare words also effectively -weighted, since may provide idiosyncratic information single document (.e., strong “prediction” one document little information rest). read Chapters 6–7 Grimmer et al., goal find right balance.Another useful tool (concept) keyness. Keyness two--two association score features occur differentially across categories. can estimate features strongly associated one category (case, one artist) relative another. Let’s compare Megan Thee Stallion Taylor Swift.Similar inferred TF-IDF graphs. Notice stemming always work expected. Taylor Swift sings “shake, shake, shake,” Megan Thee Stallion sings “shaking.” However, still appear distinct features two artists.","code":"\n# Create a quanteda corpus from the cleaned lyrics data frame.\n# - text_field specifies which column contains the text to be treated as documents.\n# - unique_docnames ensures each document gets a unique ID (useful when rows might share names/IDs).\ncorpus_lyrics <- corpus(\n  sample_lyrics,\n  text_field = \"lyrics_clean\",\n  unique_docnames = TRUE\n)\n\n# Quick overview of the corpus (number of documents, tokens, etc.)\nsummary(corpus_lyrics)## Corpus consisting of 21 documents, showing 21 documents:\n## \n##    Text Types Tokens Sentences\n##   text1   119    375        35\n##   text2    52    853        83\n##   text3   188    835        91\n##   text4    97    352        38\n##   text5   160    440        50\n##   text6   133    535        67\n##   text7   105    560        53\n##   text8    67    366        40\n##   text9    68    298        33\n##  text10    65    258        32\n##  text11   137    558        68\n##  text12   131    876        70\n##  text13   159    465        41\n##  text14   162    544        62\n##  text15   196    738        84\n##  text16   169    549        50\n##  text17   229    867        55\n##  text18   193    664        61\n##  text19   310   1190        87\n##  text20   198    656        48\n##  text21   255   1092        73\n##                    artist\n##  Rage Against the Machine\n##  Rage Against the Machine\n##  Rage Against the Machine\n##  Rage Against the Machine\n##  Rage Against the Machine\n##  Rage Against the Machine\n##          System of a Down\n##          System of a Down\n##          System of a Down\n##          System of a Down\n##          System of a Down\n##              Taylor Swift\n##              Taylor Swift\n##              Taylor Swift\n##              Taylor Swift\n##              Taylor Swift\n##       Megan Thee Stallion\n##       Megan Thee Stallion\n##       Megan Thee Stallion\n##       Megan Thee Stallion\n##       Megan Thee Stallion\n##                       album year\n##                 Evil Empire 1996\n##    Rage Against the Machine 1992\n##                   Renegades 2000\n##   The Battle of Los Angeles 1999\n##   The Battle of Los Angeles 1999\n##   The Battle of Los Angeles 1999\n##                   Mezmerize 2005\n##                    Toxicity 2001\n##                    Toxicity 2001\n##                    Toxicity 2001\n##                    Toxicity 2001\n##                        1989 2014\n##                   Midnights 2022\n##                    Fearless 2008\n##                        1989 2014\n##                    Fearless 2008\n##                  Traumazine 2022\n##                        Suga 2020\n##  Something for Thee Hotties 2021\n##                  Traumazine 2022\n##                  Traumazine 2022\n##                   song\n##        Bulls on Parade\n##    Killing in the Name\n##      Renegades of Funk\n##  Sleep Now in the Fire\n##        Guerrilla Radio\n##                Testify\n##                B.Y.O.B\n##             Chop Suey!\n##                Aerials\n##                Toxicty\n##                  Sugar\n##           Shake it Off\n##              Anti-Hero\n##     You Belong With Me\n##            Blank Space\n##             Love Story\n##                 Plan B\n##                 Savage\n##              Thot Shit\n##                    Her\n##             Ungrateful\n# Tokenize the corpus: split each document into tokens (typically words).\n# Here we remove some elements that usually add noise for word-frequency analysis.\nlyrics_toks <- tokens(\n  corpus_lyrics,\n  remove_numbers = TRUE,  # remove tokens that are numbers (are these relevant?)\n  remove_punct   = TRUE,  # remove punctuation marks (e.g., commas, periods)\n  remove_url     = TRUE   # remove URLs (useful if lyrics contain links/metadata)\n)\n\n# Inspect a couple of tokenized documents (documents 4 and 14)\nlyrics_toks[c(4, 14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"The\"     \"world\"   \"is\"     \n##  [5] \"my\"      \"expense\" \"It’s\"    \"the\"    \n##  [9] \"cost\"    \"of\"      \"my\"      \"desire\" \n## [ ... and 227 more ]\n## \n## text14 :\n##  [1] \"You're\"     \"on\"         \"the\"       \n##  [4] \"phone\"      \"with\"       \"your\"      \n##  [7] \"girlfriend\" \"she's\"      \"upset\"     \n## [10] \"She's\"      \"going\"      \"off\"       \n## [ ... and 385 more ]\n# Remove stopwords and any additional terms you want to drop before building a dfm.\n# - stopwords(language = \"en\") provides a standard English stopword list.\n# - You can add/remove terms depending on your corpus and research question.\n# - padding = FALSE drops removed tokens entirely (no placeholder tokens are kept).\nlyrics_toks <- tokens_remove(\n  lyrics_toks,\n  c(\n    stopwords(language = \"en\"),\n    # \"now\" is very frequent in this corpus (identified after inspecting the dfm),\n    # and it is not substantively useful for our purposes here.\n    \"now\"\n  ),\n  padding = FALSE\n)\n\n# Stem tokens to reduce inflected/derived words to a common root\n# (e.g., \"running\", \"runs\" -> \"run\"), which reduces vocabulary size.\nlyrics_toks_stem <- tokens_wordstem(lyrics_toks, language = \"en\")\n\n# Compare the tokenized text before and after stemming for two example documents\nlyrics_toks[c(4, 14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"world\"   \"expense\" \"It’s\"   \n##  [5] \"cost\"    \"desire\"  \"Jesus\"   \"blessed\"\n##  [9] \"future\"  \"protect\" \"fire\"    \"raise\"  \n## [ ... and 105 more ]\n## \n## text14 :\n##  [1] \"phone\"      \"girlfriend\" \"upset\"     \n##  [4] \"going\"      \"something\"  \"said\"      \n##  [7] \"Cause\"      \"get\"        \"humor\"     \n## [10] \"like\"       \"room\"       \"typical\"   \n## [ ... and 133 more ]\nlyrics_toks_stem[c(4, 14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"world\"   \"expens\"  \"It’s\"   \n##  [5] \"cost\"    \"desir\"   \"Jesus\"   \"bless\"  \n##  [9] \"futur\"   \"protect\" \"fire\"    \"rais\"   \n## [ ... and 105 more ]\n## \n## text14 :\n##  [1] \"phone\"      \"girlfriend\" \"upset\"     \n##  [4] \"go\"         \"someth\"     \"said\"      \n##  [7] \"Caus\"       \"get\"        \"humor\"     \n## [10] \"like\"       \"room\"       \"typic\"     \n## [ ... and 133 more ]\n# Create a document-feature matrix (dfm) from the tokens.\n# Rows = documents; columns = features (typically word types); cells = feature counts.\nlyrics_dfm <- dfm(lyrics_toks)\n\n# Create a dfm from the stemmed tokens to further reduce vocabulary size.\nlyrics_dfm_stem <- dfm(lyrics_toks_stem)\n\n# Inspect the first few rows/columns of the stemmed dfm\nhead(lyrics_dfm_stem)## Document-feature matrix of: 6 documents, 1,161 features (93.10% sparse) and 4 docvars.\n##        features\n## docs    come wit microphon explod shatter\n##   text1    4   4         1      1       1\n##   text2    2   0         0      0       0\n##   text3    0   0         0      0       0\n##   text4    0   0         0      0       0\n##   text5    0   0         0      0       0\n##   text6    0   4         0      0       0\n##        features\n## docs    mold either drop hit like\n##   text1    1      1    3   1    1\n##   text2    0      0    0   0    0\n##   text3    0      0    0   0    4\n##   text4    0      0    0   0    0\n##   text5    0      0    0   0    1\n##   text6    0      0    0   0    0\n## [ reached max_nfeat ... 1,151 more features ]\nlyrics_dfm_stem %>%\n  # Compute the top n most frequent features (tokens) in the dfm\n  textstat_frequency(n = 30) %>%\n  # Plot the top features as a horizontal bar chart\n  ggplot(aes(\n    x = reorder(feature, frequency),\n    y = frequency,\n    fill = frequency,\n    color = frequency\n  )) +\n  # Use bars to show counts (alpha makes them slightly transparent)\n  geom_col(alpha = 0.5) +\n  # Flip coordinates so feature labels are easier to read\n  coord_flip() +\n  # Fix ordering after coord_flip when using reorder()\n  scale_x_reordered() +\n  # Map frequency to color/fill gradients for visual emphasis\n  scale_color_distiller(palette = \"PuOr\") +\n  scale_fill_distiller(palette = \"PuOr\") +\n  # Clean theme\n  theme_minimal() +\n  labs(x = \"\", y = \"Frequency\", color = \"\", fill = \"\") +\n  # Hide legend (frequency is already shown on the y-axis)\n  theme(legend.position = \"none\")\nonly_once <- lyrics_dfm_stem %>%\n  textstat_frequency() %>%\n  filter(frequency == 1)\nlength(only_once$feature)## [1] 597\nlyrics_dfm_stem %>%\n  # Compute top features *within each artist* (grouped frequency table)\n  textstat_frequency(n = 15, groups = c(artist)) %>%\n  ggplot(aes(\n    x = reorder_within(feature, frequency, group), # reorder features separately within each facet\n    y = frequency,\n    fill = group,\n    color = group\n  )) +\n  geom_col(alpha = 0.5) +\n  coord_flip() +\n  # One panel per artist; free scales so each artist's frequency range can differ\n  facet_wrap(~group, scales = \"free\") +\n  # Fix axis ordering after reorder_within() + coord_flip()\n  scale_x_reordered() +\n  scale_color_brewer(palette = \"PuOr\") +\n  scale_fill_brewer(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x = \"\", y = \"\", color = \"\", fill = \"\") +\n  theme(legend.position = \"none\")\nlyrics_dfm_tfidf <- dfm_tfidf(lyrics_dfm_stem) # Create a dfm with tf-idf instead of counts\n\nlyrics_dfm_tfidf %>%\n  # force = TRUE ensures features are computed within groups even if some groups have sparse features\n  textstat_frequency(n = 15, groups = c(artist), force = TRUE) %>%\n  ggplot(aes(x = reorder_within(feature, frequency, group), y = frequency, fill = group, color = group)) +\n  geom_col(alpha = 0.5) +\n  coord_flip() +\n  facet_wrap(~group, scales = \"free\") +\n  scale_x_reordered() +\n  scale_color_brewer(palette = \"PuOr\") +\n  scale_fill_brewer(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x = \"\", y = \"TF-IDF\", color = \"\", fill = \"\") +\n  theme(legend.position = \"none\")\n# Subset the dfm to include only documents after 2006.\n# This is a convenient way to focus on a time period where both artists are likely represented.\nlyrics_dfm_ts_mts <- dfm_subset(lyrics_dfm_stem, year > 2006)\n\n# Compute keyness statistics (a differential association measure) for each feature.\n# - target defines the \"focus\" group: here, documents where artist == \"Taylor Swift\".\n# - The resulting object ranks features by how strongly they are associated with the target group\n#   versus the reference group (all other documents in the dfm, here: non–Taylor Swift).\nlyrics_key <- textstat_keyness(\n  lyrics_dfm_ts_mts,\n  target = lyrics_dfm_ts_mts$artist == \"Taylor Swift\"\n)\n\n# Visualize the most strongly associated (key) features for the target vs. the reference group.\ntextplot_keyness(lyrics_key)"},{"path":"week2.html","id":"word-frequency-across-artist","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.5 Word Frequency Across Artist","text":"can something similar last week look word frequencies. Rather creating dfm, can use dataset extract basic information—example, average number tokens artist.Alternatively, can estimate frequency specific token song.can now join two data frames together left_join() function using “song” column key. can pipe joined data plot.","code":"\nsample_lyrics %>%\n  # Tokenize the cleaned lyrics into one-token-per-row (similar in spirit to quanteda tokenization)\n  unnest_tokens(word, lyrics_clean) %>%\n  # Count tokens per song\n  group_by(song) %>%\n  mutate(total_tk_song = n()) %>%\n  # Keep one row per song (with its token count)\n  distinct(song, .keep_all = TRUE) %>% \n  # Compute the mean tokens per song within each artist\n  group_by(artist) %>%\n  mutate(mean_tokens = mean(total_tk_song)) %>%\n  # Plot token counts per song, faceted by artist\n  ggplot(aes(x = song, y = total_tk_song, fill = artist, color = artist)) +\n  geom_col(alpha = 0.8) +\n  # Add a dashed line for each artist's mean token count\n  geom_hline(aes(yintercept = mean_tokens, color = artist), linetype = \"dashed\") +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  scale_fill_manual(values = wes_palette(\"Royal2\")) +\n  facet_wrap(~artist, scales = \"free_x\", nrow = 1) + \n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 90, size = 5, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    x = \"\",\n    y = \"Total Tokens\",\n    color = \"\",\n    fill = \"\",\n    caption = \"Note: Dashed line shows the mean token count by artist.\"\n  )\nlyrics_totals <- sample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  group_by(song) %>%\n  mutate(total_tk_song = n()) %>%\n  distinct(song,.keep_all=T) \n# let's look for \"like\"\nlyrics_like <- sample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  filter(word==\"like\") %>%\n  group_by(song) %>%\n  mutate(total_like_song = n()) %>%\n  distinct(song,total_like_song) \nlyrics_totals %>%\n  left_join(lyrics_like, by = \"song\") %>%\n  ungroup() %>%\n  mutate(like_prop = total_like_song/total_tk_song) %>%\n  ggplot(aes(x=song,y=like_prop,fill=artist,color=artist)) +\n  geom_col(alpha=.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  scale_fill_manual(values = wes_palette(\"Royal2\")) +\n  facet_wrap(~artist, scales = \"free_x\", nrow = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 90, size = 5,vjust = 0.5, hjust=1)) +\n  labs(x=\"\", y = \"Prop. of \\'Like\\'\", color = \"\", fill = \"\")## left_join: added one column (total_like_song)\n##            > rows only in x             9\n##            > rows only in lyrics_like ( 0)\n##            > matched rows              12\n##            >                          ====\n##            > rows total                21"},{"path":"week2.html","id":"final-words-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.6 Final Words","text":"often case, won’t able cover every single feature different packages offer, show every object create, explore everything can . advice go home explore code detail. Try applying different corpus come next class questions (just show able ).","code":""},{"path":"week3.html","id":"week3","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3 Week 3: Dictionary-Based Approaches","text":"","code":""},{"path":"week3.html","id":"slides-2","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"Slides","text":"4 Dictionary-Based Approaches (link Perusall)","code":""},{"path":"week3.html","id":"setup-2","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(textdata) # text datasets\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\n# Requires installing through devtools: \n# devtools::install_github(\"kbenoit/quanteda.dictionaries\") \nlibrary(quanteda.dictionaries)\nlibrary(wesanderson) # to prettify\nlibrary(knitr) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week3.html","id":"get-data-2","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.2 Get Data:","text":"example, using data Ventura et al. (2021) - Connective effervescence streaming chat political debates.","code":"\nload(\"data/ventura_etal_df.Rdata\")\nhead(ventura_etal_df)##   text_id\n## 1       1\n## 2       2\n## 3       3\n## 4       4\n## 5       5\n## 6       6\n##                                                                                                                                                                                                                                       comments\n## 1 MORE:\\n The coronavirus pandemic's impact on the race will be on display as the\\n two candidates won't partake in a handshake, customary at the top of \\nsuch events. The size of the audience will also be limited. https://abcn.ws/3kVyl16\n## 2                                                                                                                                                           God please bless all Trump supporters. They need it for they know not what they do\n## 3                                                                                                                               Trump  is  a  living  disaster!    What  an embarrassment  to  all  human  beings!    The  man  is  dangerous!\n## 4                                                                                                                                                   This debate is why other counties laugh at us. School yard class president debate at best.\n## 5                                                                    OMG\\n ... shut up tRump ... so rude and out of control.  Obviously freaking \\nout.  This is a debate NOT a convention or a speech or your platform.  \\nLearn some manners\n## 6                                                                                                      It’s\\n hard to see what this country has become.  The Presidency is no longer a\\n respected position it has lost all of it’s integrity.\n##                      id likes\n## 1              ABC News   100\n## 2            Anita Hill    61\n## 3          Dave Garland    99\n## 4              Carl Roy    47\n## 5 Lynda Martin-Chambers   154\n## 6         Nica Merchant   171\n##                    debate\n## 1 abc_first_debate_manual\n## 2 abc_first_debate_manual\n## 3 abc_first_debate_manual\n## 4 abc_first_debate_manual\n## 5 abc_first_debate_manual\n## 6 abc_first_debate_manual"},{"path":"week3.html","id":"tokenization-etc.","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.3 Tokenization etc.","text":"comments mostly clean, can check () whether require additional cleaning. previous code, showed lowercase text, remove stopwords, etc., using quanteda. can also using tidytext3:","code":"\n# Tokenize and lightly clean the `comments` text using tidytext.\n#\n# What this does:\n# 1) lowercases the text in `comments`\n# 2) tokenizes into one-token-per-row using `unnest_tokens()`\n# 3) keeps only tokens that contain at least one lowercase letter (regex check)\n# 4) removes stopwords using the `stop_words` lexicon\n\ntidy_ventura <- ventura_etal_df %>% \n  # Lowercase the raw comment text to standardize tokens (e.g., \"Trump\" -> \"trump\")\n  mutate(comments = tolower(comments)) %>%\n  # Tokenize: creates a new column `word` and drops the original `comments` column by default\n  unnest_tokens(word, comments) %>%\n  # Keep only tokens that contain at least one lowercase letter a–z\n  # (this removes many punctuation-only / number-only tokens; tweak as needed)\n  filter(str_detect(word, \"[a-z]\")) %>%\n  # Remove stopwords (common function words like \"the\", \"and\", etc.)\n  filter(!word %in% stop_words$word)\n\nhead(tidy_ventura, 20)##    text_id         id likes\n## 1        1   ABC News   100\n## 2        1   ABC News   100\n## 3        1   ABC News   100\n## 4        1   ABC News   100\n## 5        1   ABC News   100\n## 6        1   ABC News   100\n## 7        1   ABC News   100\n## 8        1   ABC News   100\n## 9        1   ABC News   100\n## 10       1   ABC News   100\n## 11       1   ABC News   100\n## 12       1   ABC News   100\n## 13       1   ABC News   100\n## 14       1   ABC News   100\n## 15       1   ABC News   100\n## 16       1   ABC News   100\n## 17       1   ABC News   100\n## 18       2 Anita Hill    61\n## 19       2 Anita Hill    61\n## 20       2 Anita Hill    61\n##                     debate        word\n## 1  abc_first_debate_manual coronavirus\n## 2  abc_first_debate_manual  pandemic's\n## 3  abc_first_debate_manual      impact\n## 4  abc_first_debate_manual        race\n## 5  abc_first_debate_manual     display\n## 6  abc_first_debate_manual  candidates\n## 7  abc_first_debate_manual     partake\n## 8  abc_first_debate_manual   handshake\n## 9  abc_first_debate_manual   customary\n## 10 abc_first_debate_manual         top\n## 11 abc_first_debate_manual      events\n## 12 abc_first_debate_manual        size\n## 13 abc_first_debate_manual    audience\n## 14 abc_first_debate_manual     limited\n## 15 abc_first_debate_manual       https\n## 16 abc_first_debate_manual     abcn.ws\n## 17 abc_first_debate_manual     3kvyl16\n## 18 abc_first_debate_manual         god\n## 19 abc_first_debate_manual       bless\n## 20 abc_first_debate_manual       trump"},{"path":"week3.html","id":"keywords","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.4 Keywords","text":"can detect occurrence words trump biden comment (text_id).Rather replicating results Figure 3 Ventura et al. (2021), estimate median number likes comments mentioning Trump, Biden, , neither receive:can also see differences across news media:Similar Young Soroka (2012), can also explore keywords interest context. good way validate proposed measure (e.g., mentioning trump reflection interest, simply relevance?).can also look one word time:Alternatively, can examine words commonly occur together. called collocations (concept closely related n-grams). , want identify common names mentioned (first last names).(\\(\\lambda\\) score measure strongly two words associated, example, likely chris wallace occur next . complete explanation, can read paper.)can also discover collocations longer two words. example , identify collocations consisting three words.","code":"\n# Create a comment-level dataset indicating whether each comment mentions \"trump\" and/or \"biden\" at least once.\n\ntrump_biden <- tidy_ventura %>%\n  # Create token-level indicator variables (1 if the token is the target word, else 0)\n  mutate(\n    trump_token = ifelse(word == \"trump\", 1, 0),\n    biden_token = ifelse(word == \"biden\", 1, 0)\n  ) %>%\n  # Aggregate within each comment (grouped by text_id) to mark whether the comment\n  # contains at least one occurrence of each target word\n  group_by(text_id) %>%\n  mutate(\n    trump_cmmnt = ifelse(sum(trump_token) > 0, 1, 0),\n    biden_cmmnt = ifelse(sum(biden_token) > 0, 1, 0)\n  ) %>%\n  # Reduce to the unit of analysis: one row per comment (text_id)\n  distinct(text_id, .keep_all = TRUE) %>%\n  dplyr::select(text_id, trump_cmmnt, biden_cmmnt, likes, debate)\n\nhead(trump_biden, 20)## # A tibble: 20 × 5\n## # Groups:   text_id [20]\n##    text_id trump_cmmnt biden_cmmnt likes\n##      <int>       <dbl>       <dbl> <int>\n##  1       1           0           0   100\n##  2       2           1           0    61\n##  3       3           1           0    99\n##  4       4           0           0    47\n##  5       5           1           0   154\n##  6       6           0           0   171\n##  7       7           0           0    79\n##  8       8           0           0    39\n##  9       9           0           0    53\n## 10      10           0           0    36\n## 11      11           1           0    41\n## 12      12           0           0    28\n## 13      13           1           0    54\n## 14      14           0           0    30\n## 15      15           1           0    27\n## 16      16           1           1    31\n## 17      17           1           0    35\n## 18      18           1           1    32\n## 19      19           0           0    34\n## 20      20           1           0    37\n## # ℹ 1 more variable: debate <chr>\ntrump_biden %>%\n  # Create categories\n  mutate(mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==0, \"1. None\", NA),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==0, \"2. Trump\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==1, \"3. Biden\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==1, \"4. Both\", mentions_cat)) %>%\n  # Remove the ones people like too much\n  filter(likes < 26) %>%\n  group_by(mentions_cat) %>%\n  mutate(likes_mean = median(likes, na.rm = T)) %>%\n  ungroup() %>%\n  # Plot densities by category, with dashed median lines\n  ggplot(aes(x=likes,fill = mentions_cat, color = mentions_cat)) +\n  geom_density(alpha = 0.3) +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~mentions_cat, ncol = 1) + \n  theme_minimal() +\n  geom_vline(aes(xintercept = likes_mean, color = mentions_cat), linetype = \"dashed\")+\n  theme(legend.position=\"none\") +\n  labs(x=\"\", y = \"Density\", color = \"\", fill = \"\",\n       caption = \"Note: Median likes in dashed lines.\")\ntrump_biden %>%\n  # Create categories\n  mutate(mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==0, \"1. None\", NA),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==0, \"2. Trump\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==1, \"3. Biden\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==1, \"4. Both\", mentions_cat),\n         media = ifelse(str_detect(debate, \"abc\"), \"ABC\", NA),\n         media = ifelse(str_detect(debate, \"nbc\"), \"NBC\", media),\n         media = ifelse(str_detect(debate, \"fox\"), \"FOX\", media)) %>%\n  # Remove the ones people like too much\n  filter(likes < 26) %>%\n  group_by(mentions_cat,media) %>%\n  mutate(median_like = median(likes,na.rm = T)) %>%\n  ungroup() %>%\n  # Plot\n  ggplot(aes(x=likes,fill = mentions_cat, color = mentions_cat)) +\n  geom_density(alpha = 0.3) +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~media, ncol = 1) + \n  geom_vline(aes(xintercept = median_like, color = mentions_cat), linetype = \"dashed\")+\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(x=\"\", y = \"Density\", color = \"\", fill = \"\",\n       caption = \"Note: Median likes in dashed lines.\")\n# Create a quanteda corpus from the Ventura et al. dataset.\n# - text_field indicates which column contains the text to treat as documents.\n# - unique_docnames ensures each document is assigned a unique ID.\ncorpus_ventura <- corpus(\n  ventura_etal_df,\n  text_field = \"comments\",\n  unique_docnames = TRUE\n)\n\n# Tokenize the corpus so we can use token-based tools like kwic()\ntoks_ventura <- tokens(corpus_ventura)\n\n# Extract \"keywords in context\" (KWIC) for occurrences of \"Trump\"\n# This returns the keyword plus a window of surrounding tokens.\nkw_trump <- kwic(toks_ventura, pattern = \"Trump\")\n\n# Inspect the first 20 KWIC results\n# (The number of tokens before/after the keyword is controlled by the window size in kwic().)\nhead(kw_trump, 20)## Keyword-in-context with 20 matches.                                         \n##    [text2, 5]      God please bless all |\n##    [text3, 1]                           |\n##    [text5, 7]               ... shut up |\n##  [text11, 11] a bad opiate problem then |\n##   [text13, 4]                 This is a |\n##   [text15, 1]                           |\n##   [text16, 8]  this SO much better than |\n##   [text17, 3]                    I love |\n##   [text18, 4]            Biden is right |\n##   [text20, 1]                           |\n##  [text22, 12]     being a decent human. |\n##   [text23, 1]                           |\n##  [text27, 11]          for once, i wish |\n##  [text28, 10]             it America... |\n##   [text30, 1]                           |\n##   [text31, 1]                           |\n##   [text32, 1]                           |\n##  [text32, 15]    People open your eyes. |\n##   [text34, 1]                           |\n##   [text36, 1]                           |\n##                                         \n##  Trump | supporters. They need it       \n##  Trump | is a living disaster!          \n##  tRump | ... so rude                    \n##  trump | brings up about bidens son     \n##  TRUMP | all about ME debate and        \n##  Trump | is looking pretty flushed right\n##  Trump | and I wasn’t even going        \n##  Trump | ! He is the best               \n##  Trump | doesn’t have a plan for        \n##  Trump | worse president EVER 😡 thank  \n##  Trump | doesn't know the meaning of    \n##  Trump | such a hateful person he       \n##  trump | would shut his trap for        \n##  Trump | IS NOT smarter than a          \n##  Trump | has improved our economy and   \n##  Trump | has done so much harm          \n##  Trump | is a clown and after           \n##  Trump | is evil.                       \n##  Trump | is so broke that is            \n##  Trump | is literally making this debate\nkw_best <- kwic(toks_ventura, pattern = c(\"best\",\"worst\"))\nhead(kw_best, 20)## Keyword-in-context with 20 matches.                                             \n##    [text4, 17] yard class president debate at\n##    [text10, 1]                               \n##    [text17, 8]               Trump! He is the\n##    [text43, 6]           This is gonna be the\n##   [text81, 31]  an incompetent President, the\n##   [text81, 33]          President, the worst,\n##   [text81, 35]              the worst, worst,\n##   [text82, 11]         was totally one sided!\n##    [text86, 8]           right - Trump is the\n##   [text100, 9]             !! BRAVO BRAVO THE\n##   [text102, 4]                  Obama was the\n##  [text119, 10]            he said he would do\n##  [text138, 13]               think. He is the\n##  [text141, 22]           puppet could be? The\n##   [text143, 6]           Trump may not be the\n##  [text158, 15]              This man is a the\n##   [text167, 3]                         He the\n##  [text221, 34]           by far have been the\n##  [text221, 36]           have been the worst,\n##  [text221, 38]              the worst, WORST,\n##           \n##  | best  |\n##  | Worst |\n##  | best  |\n##  | best  |\n##  | worst |\n##  | worst |\n##  | worst |\n##  | Worst |\n##  | worst |\n##  | BEST  |\n##  | worst |\n##  | Best  |\n##  | worst |\n##  | worst |\n##  | best  |\n##  | worst |\n##  | worst |\n##  | worst |\n##  | WORST |\n##  | WORST |\n##                                     \n##  .                                  \n##  debate I’ve ever seen!             \n##  president ever! Thank you          \n##  show on TV in 4                    \n##  , worst, worst in                  \n##  , worst in history.                \n##  in history.                        \n##  ever! Our president kept           \n##  president America ever had!        \n##  PRESIDENT OF THE WORLD.            \n##  president ever!!!                  \n##  President ever Crybabies don't like\n##  president ever                     \n##  president in our time ever         \n##  choice but I will choose           \n##  thing that has ever happened       \n##  president we had in the            \n##  , WORST, WORST PRESIDENT           \n##  , WORST PRESIDENT!!                \n##  PRESIDENT!!!\ntoks_ventura <- tokens(corpus_ventura, remove_punct = TRUE)\ncol_ventura <- tokens_select(toks_ventura, \n                                # Keep only tokens that start with a capital letter\n                                pattern = \"^[A-Z]\", \n                                valuetype = \"regex\", \n                                case_insensitive = FALSE, \n                                padding = TRUE) %>% \n                  textstat_collocations(min_count = 20) # Minimum number of collocations to be taken into account.\nhead(col_ventura, 20)##          collocation count count_nested\n## 1      chris wallace  1695            0\n## 2    president trump   831            0\n## 3          joe biden   431            0\n## 4           fox news   267            0\n## 5       mr president   152            0\n## 6      united states   144            0\n## 7       donald trump   141            0\n## 8         mike pence    40            0\n## 9       jo jorgensen    78            0\n## 10             HE IS    43            0\n## 11  democratic party    38            0\n## 12    vice president   347            0\n## 13     CHRIS WALLACE    38            0\n## 14   PRESIDENT TRUMP    37            0\n## 15          TRUMP IS    42            0\n## 16       white house    47            0\n## 17 african americans    35            0\n## 18         JOE BIDEN    25            0\n## 19           YOU ARE    27            0\n## 20            IS NOT    34            0\n##    length    lambda         z\n## 1       2  6.752252 128.27065\n## 2       2  3.747948  84.12773\n## 3       2  3.389153  59.43303\n## 4       2  8.943930  53.78327\n## 5       2  4.985057  45.89307\n## 6       2 12.106819  36.13493\n## 7       2  4.735894  35.48376\n## 8       2  8.952895  34.74457\n## 9       2 10.969720  34.45690\n## 10      2  6.205826  34.13557\n## 11      2  9.093924  31.88407\n## 12      2  8.555551  31.81775\n## 13      2  9.634400  31.78949\n## 14      2  5.845521  30.56607\n## 15      2  5.197700  30.15608\n## 16      2 11.387847  29.41946\n## 17      2  7.750170  29.38751\n## 18      2  7.534616  28.84871\n## 19      2  6.651381  28.81572\n## 20      2  5.508680  28.73548\n# Identify frequent collocations (multi-word expressions) in the Ventura corpus.\n# - tokens_select() is used here mainly to ensure settings are explicit:\n#   * case_insensitive = FALSE keeps case distinctions (e.g., \"Trump\" vs \"trump\")\n#   * padding = TRUE preserves token positions so collocations can be detected properly\n# - textstat_collocations() finds statistically associated token sequences.\n#   * min_count sets a minimum frequency threshold (here: at least 100 occurrences)\n#   * size = 3 searches for 3-word collocations (trigrams)\ncol_ventura <- tokens_select(\n  toks_ventura,\n  case_insensitive = FALSE,\n  padding = TRUE\n) %>%\n  textstat_collocations(min_count = 100, size = 3)\n\n# Inspect the top 20 collocations\nhead(col_ventura, 20)##            collocation count count_nested\n## 1          know how to   115            0\n## 2  the american people   220            0\n## 3          this is the   158            0\n## 4           to do with   108            0\n## 5       this debate is   167            0\n## 6             is not a   139            0\n## 7     wallace needs to   172            0\n## 8         is the worst   110            0\n## 9            is such a   107            0\n## 10        trump is the   153            0\n## 11           is a joke   248            0\n## 12      trump has done   105            0\n## 13          trump is a   322            0\n## 14         this is not   119            0\n## 15      trump needs to   131            0\n## 16         what a joke   141            0\n## 17   the united states   132            0\n## 18         going to be   122            0\n## 19         is going to   210            0\n## 20          biden is a   164            0\n##    length      lambda           z\n## 1       3 3.098608190 11.32651308\n## 2       3 2.602503749 10.16184037\n## 3       3 1.393392161  9.01490638\n## 4       3 4.010890138  7.21665079\n## 5       3 0.994328169  6.14171313\n## 6       3 0.796789305  6.08626604\n## 7       3 1.635032948  4.63118161\n## 8       3 1.840376131  3.63917670\n## 9       3 0.776121492  2.54094422\n## 10      3 0.280826175  2.53147919\n## 11      3 2.096508909  2.53031226\n## 12      3 0.644366045  2.27854315\n## 13      3 0.204080417  2.01355366\n## 14      3 0.447286359  1.99058988\n## 15      3 0.577684570  1.93126531\n## 16      3 2.376681732  1.67038242\n## 17      3 0.738270253  1.43145851\n## 18      3 1.918098701  1.35112904\n## 19      3 0.101248917  0.60225753\n## 20      3 0.001248882  0.01013271"},{"path":"week3.html","id":"dictionary-approaches","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5 Dictionary Approaches","text":"can extend previous analysis using dictionaries. can create , use previously validated dictionaries, use dictionaries already included tidytext quanteda (e.g., sentiment analysis).","code":""},{"path":"week3.html","id":"sentiment-analysis","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5.1 Sentiment Analysis","text":"Let’s look pre-loaded sentiment dictionaries tidytext:AFFIN: measures sentiment numeric score -5 5, validated paper.bing: sentiment words found online forums. information .nrc: underpaid workers Amazon mechanical Turk coded emotional valence long list terms, validated paper. Also, check paper MTurk, shouldn’t trust data collected MTurk.dictionary classifies quantifies words different way. Let’s use nrc sentiment dictionary analyze comments dataset. nrc dictionary classifies, among concepts, words reflecting positive negative sentiment.focus solely positive negative sentiment:Let’s check top positive words top negative words:classifications make intuitive sense: “love” positive “bully” negative. Others less convincing: “talk” positive? “joke” negative? depend heavily context: “vice” negative, vice president typically (especially since “president” classified “positive,” … really?). “vote” positive negative, … ?Let’s turn blind eye now (, , see Grimmer et al., Chapter 15 best practices).people watch different news media use different language? Let’s see data tell us. always, check unit analysis dataset. case, observation word, grouping variable comment (text_id), can count many positive negative words appear comment. calculate net sentiment score subtracting number negative words number positive words (within comment).Ok, now can plot differences:","code":"\nget_sentiments(\"afinn\")## # A tibble: 2,477 × 2\n##    word       value\n##    <chr>      <dbl>\n##  1 abandon       -2\n##  2 abandoned     -2\n##  3 abandons      -2\n##  4 abducted      -2\n##  5 abduction     -2\n##  6 abductions    -2\n##  7 abhor         -3\n##  8 abhorred      -3\n##  9 abhorrent     -3\n## 10 abhors        -3\n## # ℹ 2,467 more rows\nget_sentiments(\"bing\")## # A tibble: 6,786 × 2\n##    word        sentiment\n##    <chr>       <chr>    \n##  1 2-faces     negative \n##  2 abnormal    negative \n##  3 abolish     negative \n##  4 abominable  negative \n##  5 abominably  negative \n##  6 abominate   negative \n##  7 abomination negative \n##  8 abort       negative \n##  9 aborted     negative \n## 10 aborts      negative \n## # ℹ 6,776 more rows\nget_sentiments(\"nrc\")## # A tibble: 13,872 × 2\n##    word        sentiment\n##    <chr>       <chr>    \n##  1 abacus      trust    \n##  2 abandon     fear     \n##  3 abandon     negative \n##  4 abandon     sadness  \n##  5 abandoned   anger    \n##  6 abandoned   fear     \n##  7 abandoned   negative \n##  8 abandoned   sadness  \n##  9 abandonment anger    \n## 10 abandonment fear     \n## # ℹ 13,862 more rows\nnrc <- get_sentiments(\"nrc\") \ntable(nrc$sentiment)## \n##        anger anticipation      disgust \n##         1245          837         1056 \n##         fear          joy     negative \n##         1474          687         3316 \n##     positive      sadness     surprise \n##         2308         1187          532 \n##        trust \n##         1230\nnrc_pos_neg <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"positive\" | sentiment == \"negative\")\n\nventura_pos_neg <- tidy_ventura %>%\n  left_join(nrc_pos_neg)## Joining with `by = join_by(word)`\n## left_join: added one column (sentiment)\n## > rows only in x 147,204\n## > rows only in nrc_pos_neg ( 3,402)\n## > matched rows 52,059 (includes duplicates)\n## > =========\n## > rows total 199,263\nventura_pos_neg %>%\n  group_by(sentiment) %>%\n  count(word, sort = TRUE)## count: now 14,242 rows and 3 columns, one\n## group variable remaining (sentiment)## # A tibble: 14,242 × 3\n## # Groups:   sentiment [3]\n##    sentiment word          n\n##    <chr>     <chr>     <int>\n##  1 <NA>      trump     11676\n##  2 <NA>      biden      7847\n##  3 positive  president  4920\n##  4 <NA>      wallace    4188\n##  5 positive  debate     2693\n##  6 <NA>      people     2591\n##  7 <NA>      chris      2559\n##  8 <NA>      joe        2380\n##  9 <NA>      country    1589\n## 10 <NA>      time       1226\n## # ℹ 14,232 more rows\ncomment_pos_neg <- ventura_pos_neg %>%\n  # Create dummies of pos and neg for counting\n  mutate(pos_dum = ifelse(sentiment == \"positive\", 1, 0),\n         neg_dum = ifelse(sentiment == \"negative\", 1, 0)) %>%\n  # Estimate total number of tokens per comment, pos , and negs\n  group_by(text_id) %>%\n  mutate(total_words = n(),\n         total_pos = sum(pos_dum, na.rm = T),\n         total_neg = sum(neg_dum, na.rm = T)) %>%\n  # These values are aggregated at the text_id level so we can eliminate repeated text_id\n  distinct(text_id,.keep_all=TRUE) %>%\n  # Now we estimate the net sentiment score. You can change this and get a different way to measure the ratio of positive to negative\n  mutate(net_sent = total_pos - total_neg) %>%\n  ungroup() \n  \n# Note that the `word` and `sentiment` columns are meaningless now\nhead(comment_pos_neg, 10)## # A tibble: 10 × 12\n##    text_id id     likes debate word  sentiment\n##      <int> <chr>  <int> <chr>  <chr> <chr>    \n##  1       1 ABC N…   100 abc_f… coro… <NA>     \n##  2       2 Anita…    61 abc_f… god   positive \n##  3       3 Dave …    99 abc_f… trump <NA>     \n##  4       4 Carl …    47 abc_f… deba… positive \n##  5       5 Lynda…   154 abc_f… omg   <NA>     \n##  6       6 Nica …   171 abc_f… it’s  <NA>     \n##  7       7 Conni…    79 abc_f… happ… <NA>     \n##  8       8 Tammy…    39 abc_f… expe… <NA>     \n##  9       9 Susan…    53 abc_f… smart <NA>     \n## 10      10 Dana …    36 abc_f… worst <NA>     \n## # ℹ 6 more variables: pos_dum <dbl>,\n## #   neg_dum <dbl>, total_words <int>,\n## #   total_pos <dbl>, total_neg <dbl>,\n## #   net_sent <dbl>\ncomment_pos_neg %>%\n    # Create categories\n  mutate(media = ifelse(str_detect(debate, \"abc\"), \"ABC\", NA),\n         media = ifelse(str_detect(debate, \"nbc\"), \"NBC\", media),\n         media = ifelse(str_detect(debate, \"fox\"), \"FOX\", media)) %>%\n  group_by(media) %>%\n  mutate(median_sent = mean(net_sent)) %>%\n  ggplot(aes(x=net_sent,color=media,fill=media)) +\n  geom_histogram(alpha = 0.4,\n                 binwidth = 1) +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~media, ncol = 1) + \n  geom_vline(aes(xintercept = median_sent, color = media), linetype = \"dashed\")+\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  coord_cartesian(xlim = c(-5,5)) +\n  labs(x=\"\", y = \"Count\", color = \"\", fill = \"\",\n       caption = \"Note: Mean net sentiment in dashed lines.\")"},{"path":"week3.html","id":"domain-specific-dictionaries","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5.2 Domain-Specific Dictionaries","text":"Sentiment dictionaries common, can build dictionary concept ’re interested . long can create lexicon (validate ), can conduct analysis similar one just carried . time, rather using --shelf sentiment dictionary, create . Let’s try dictionary two topics: economy migration.long dictionary structure nrc_pos_neg object, can follow process used sentiment dictionary.Let’s see find words comments:many. Note stem lemmatize corpus, want capture “job” “jobs,” need include dictionary. words, preprocessing steps apply corpus also applied dictionary.bit versed R, notice dictionaries often represented lists. quanteda understands dictionaries lists, can build way use liwcalike() function find matching words text. added benefit can use glob patterns capture variations word (e.g., job* match “job,” “jobs,” “jobless”).output provides interesting information. First, economy immigration give us percentage words text match economy immigration dictionaries. general, expect many words sentence reference, example, “jobs” us conclude sentence economy. , value 0% can interpreted mentioning economy (unless theoretical reasons treat, say, 3% meaningfully different 2%). remaining variables:WPS: Words per sentence.WC: Word count.Sixltr: Six-letter words (%).Dic: % words dictionary.Allpunct: % punctuation marks.Period OtherP: % specific punctuation marks.information, can identify users focus topic:Table 3.1: % mentions topic media outlet.","code":"\n# Define two simple, domain-specific dictionaries (lexicons) for:\n# 1) the economy and 2) migration.\n# Each dictionary is a two-column data frame with:\n# - word:  the token to match in the text\n# - topic: the category/label we want to assign when that token appears\n\neconomy <- cbind.data.frame(\n  c(\"economy\", \"taxes\", \"inflation\", \"debt\", \"employment\", \"jobs\"),\n  \"economy\"\n)\ncolnames(economy) <- c(\"word\", \"topic\")\n\nmigration <- cbind.data.frame(\n  c(\"immigrants\", \"border\", \"wall\", \"alien\", \"migrant\", \"visa\", \"daca\", \"dreamer\"),\n  \"migration\"\n)\ncolnames(migration) <- c(\"word\", \"topic\")\n\n# Combine the two topic-specific lexicons into a single dictionary object\ndict <- rbind.data.frame(economy, migration)\n\n# Inspect the resulting dictionary\ndict##          word     topic\n## 1     economy   economy\n## 2       taxes   economy\n## 3   inflation   economy\n## 4        debt   economy\n## 5  employment   economy\n## 6        jobs   economy\n## 7  immigrants migration\n## 8      border migration\n## 9        wall migration\n## 10      alien migration\n## 11    migrant migration\n## 12       visa migration\n## 13       daca migration\n## 14    dreamer migration\nventura_topic <- tidy_ventura %>%\n  left_join(dict)## Joining with `by = join_by(word)`\n## left_join: added one column (topic)\n## > rows only in x 196,175\n## > rows only in dict ( 3)\n## > matched rows 1,373\n## > =========\n## > rows total 197,548\nventura_topic %>%\n  filter(!is.na(topic)) %>%\n  group_by(topic) %>%\n  count(word, sort = TRUE)## count: now 11 rows and 3 columns, one group\n## variable remaining (topic)## # A tibble: 11 × 3\n## # Groups:   topic [2]\n##    topic     word           n\n##    <chr>     <chr>      <int>\n##  1 economy   taxes        680\n##  2 economy   economy      328\n##  3 economy   jobs         273\n##  4 migration wall          34\n##  5 economy   debt          32\n##  6 migration immigrants    12\n##  7 migration border         7\n##  8 economy   employment     3\n##  9 migration alien          2\n## 10 migration daca           1\n## 11 migration visa           1\ndict <- dictionary(list(economy = c(\"econom*\",\"tax*\",\"inflation\",\"debt*\",\"employ*\",\"job*\"),\n                        immigration = c(\"immigrant*\",\"border\",\"wall\",\"alien\",\"migrant*\",\"visa*\",\"daca\",\"dreamer*\"))) \n\n# liwcalike lowercases input text\nventura_topics <- liwcalike(ventura_etal_df$comments,\n                               dictionary = dict)\n\n# liwcalike keeps the order so we can cbind them directly\ntopics <- cbind.data.frame(ventura_etal_df,ventura_topics) \n\n# Look only at the comments that mention the economy and immigration\nhead(topics[topics$economy>0 & topics$immigration>0,])##       text_id\n## 4998     4999\n## 6475     6477\n## 8098     8113\n## 12331   32211\n## 14345   34225\n## 19889   62164\n##                                                                                                                                              comments\n## 4998                           Trump is going to create jobs to finish that wall,  hows that working for ya?  I don’t see Mexico paying for it either\n## 6475                           Trump is trash illegal immigrants pay more taxes than this man and you guys support this broke failure con billionaire\n## 8098                                  $750.00 in taxes in two years?????   BUT HE'S ALL OVER THE PLACE INSULTING IMMIGRANTS WHO PAID MORE IN TAXES!!!\n## 12331    Ask\\n Biden how much he will raise taxes to pay for all the things he says he\\n is going to provide everyone - including illegal immigrants!\n## 14345 Trump has been living the life and does not care for the hard working American...His taxes are not the only rip off...Investigate Wall Money...\n## 19889                                                               Vote trump out. He needs to pay taxes too ... immigrants pay more than that thief\n##                        id likes\n## 4998         Ellen Lustic    NA\n## 6475      Kevin G Vazquez     1\n## 8098      Prince M Dorbor     1\n## 12331 Lynne Basista Shine     6\n## 14345          RJ Jimenez     4\n## 19889      Nicole Brennan    13\n##                        debate   docname\n## 4998  abc_first_debate_manual  text4998\n## 6475  abc_first_debate_manual  text6475\n## 8098  abc_first_debate_manual  text8098\n## 12331 fox_first_debate_manual text12331\n## 14345 fox_first_debate_manual text14345\n## 19889 nbc_first_debate_manual text19889\n##       Segment      WPS WC Sixltr   Dic\n## 4998     4998 12.50000 25   4.00  8.00\n## 6475     6475 20.00000 20  25.00 10.00\n## 8098     8098 14.00000 28   7.14 10.71\n## 12331   12331 27.00000 27  18.52  7.41\n## 14345   14345 11.66667 35   8.57  5.71\n## 19889   19889  9.50000 19   5.26 10.53\n##       economy immigration AllPunc Period\n## 4998     4.00        4.00   12.00   0.00\n## 6475     5.00        5.00    0.00   0.00\n## 8098     7.14        3.57   35.71   3.57\n## 12331    3.70        3.70    7.41   0.00\n## 14345    2.86        2.86   25.71  25.71\n## 19889    5.26        5.26   21.05  21.05\n##       Comma Colon SemiC QMark Exclam Dash\n## 4998      4     0     0  4.00   0.00  0.0\n## 6475      0     0     0  0.00   0.00  0.0\n## 8098      0     0     0 17.86  10.71  0.0\n## 12331     0     0     0  0.00   3.70  3.7\n## 14345     0     0     0  0.00   0.00  0.0\n## 19889     0     0     0  0.00   0.00  0.0\n##       Quote Apostro Parenth OtherP\n## 4998   4.00    4.00       0   8.00\n## 6475   0.00    0.00       0   0.00\n## 8098   3.57    3.57       0  35.71\n## 12331  0.00    0.00       0   3.70\n## 14345  0.00    0.00       0  25.71\n## 19889  0.00    0.00       0  21.05"},{"path":"week3.html","id":"using-pre-built-dictionaries","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5.3 Using Pre-Built Dictionaries","text":"far, seen apply pre-loaded dictionaries (e.g., sentiment) apply dictionaries. pre-built dictionary want apply corpus? long dictionary correct structure, can use techniques applied far. also means may need data wrangling, since pre-built dictionaries come many formats.Let’s use NRC Affect Intensity Lexicon (created team behind pre-loaded nrc sentiment dictionary). NRC Affect Intensity Lexicon measures intensity emotion scale 0 (low) 1 (high). example, “defiance” anger intensity 0.51, “hate” anger intensity 0.83.simple dictionary: key advantage provides intensity score word, gives us variation analysis (e.g., instead binary anger/-anger measure, can analyze degrees anger). use tidytext approach analyze degrees “joy” corpus.Now, can see relationship likes joy:Finally, sake showing process, write code load dictionary using quanteda, note approach loses intensity information.","code":"\nintense_lex <- read.table(file = \"data/NRC-AffectIntensity-Lexicon.txt\", fill = TRUE,\n                          header = TRUE)\nhead(intense_lex)##         term score AffectDimension\n## 1   outraged 0.964           anger\n## 2  brutality 0.959           anger\n## 3     hatred 0.953           anger\n## 4    hateful 0.940           anger\n## 5  terrorize 0.939           anger\n## 6 infuriated 0.938           anger\njoy_lex <- intense_lex %>%\n  filter(AffectDimension==\"joy\") %>%\n  mutate(word=term) %>%\n  dplyr::select(word,AffectDimension,score)\n\nventura_joy <- tidy_ventura %>%\n  left_join(joy_lex) %>%\n  ## Most of the comments have no joy words so we will change these NAs to 0 but this is an ad-hoc decision. This decision must be theoretically motivated and justified\n  mutate(score = ifelse(is.na(score),0,score))## Joining with `by = join_by(word)`\n## left_join: added 2 columns (AffectDimension,\n## score)\n## > rows only in x 184,943\n## > rows only in joy_lex ( 769)\n## > matched rows 12,605\n## > =========\n## > rows total 197,548\nhead(ventura_joy[ventura_joy$score>0,])##    text_id           id likes\n## 18       2   Anita Hill    61\n## 19       2   Anita Hill    61\n## 23       3 Dave Garland    99\n## 30       4     Carl Roy    47\n## 64       8  Tammy Eisen    39\n## 65       8  Tammy Eisen    39\n##                     debate       word\n## 18 abc_first_debate_manual        god\n## 19 abc_first_debate_manual      bless\n## 23 abc_first_debate_manual     living\n## 30 abc_first_debate_manual      laugh\n## 64 abc_first_debate_manual experience\n## 65 abc_first_debate_manual      share\n##    AffectDimension score\n## 18             joy 0.545\n## 19             joy 0.561\n## 23             joy 0.312\n## 30             joy 0.891\n## 64             joy 0.375\n## 65             joy 0.438\nlibrary(MASS) # To add the negative binomial fitted line\n\nventura_joy %>%\n  mutate(media = ifelse(str_detect(debate, \"abc\"), \"ABC\", NA),\n         media = ifelse(str_detect(debate, \"nbc\"), \"NBC\", media),\n         media = ifelse(str_detect(debate, \"fox\"), \"FOX\", media)) %>%\n  # Calculate mean joy in each comment\n  group_by(text_id) %>%\n  mutate(mean_joy = mean(score)) %>%\n  distinct(text_id,mean_joy,likes,media) %>%\n  ungroup() %>%\n  # Let's only look at comments that had SOME joy in them\n  filter(mean_joy > 0) %>%\n  # Remove the ones people like too much\n  filter(likes < 26) %>%\n  # Plot\n  ggplot(aes(x=mean_joy,y=likes,color=media,fill=media)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"glm.nb\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~media, ncol = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\") +\n  labs(x=\"Mean Joy\", y = \"Likes\", color = \"\", fill = \"\")## `geom_smooth()` using formula = 'y ~ x'\naffect_dict <- dictionary(list(anger = intense_lex$term[intense_lex$AffectDimension==\"anger\"],\n                        fear = intense_lex$term[intense_lex$AffectDimension==\"fear\"],\n                        joy = intense_lex$term[intense_lex$AffectDimension==\"joy\"],\n                        sadness = intense_lex$term[intense_lex$AffectDimension==\"sadness\"])) \n\nventura_affect <- liwcalike(ventura_etal_df$comments,\n                               dictionary = affect_dict)\n\n# liwcalike keeps the order so we can cbind them directly\naffect <- cbind.data.frame(ventura_etal_df,ventura_affect) \n\n# Look only at the comments that have anger and fear\nhead(affect[affect$anger>0 & affect$fear>0,])##    text_id\n## 3        3\n## 7        7\n## 9        9\n## 11      11\n## 12      12\n## 23      23\n##                                                                                                                                                                                       comments\n## 3                                                                               Trump  is  a  living  disaster!    What  an embarrassment  to  all  human  beings!    The  man  is  dangerous!\n## 7                                                                                  What happened to the days when it was a debate not a bully session! I am so ashamed of this administration!\n## 9  ......\\n a smart president?  A thief, a con man, and a liar that has taken tax \\npayers money to his own properties.  A liar that knew the magnitude of \\nthe virus and did not address it.\n## 11                             with\\n the usa having such a bad opiate problem then trump brings up about \\nbidens son is the most disgraceful thing any human being could do...vote\\n him out\n## 12   Trump’s\\n only recourse in the debate is to demean his opponent and talk about \\nwhat a great man he, himself is. Turn his mic off when it’s not his turn\\n to speak. Nothing but babble!\n## 23                                                                                           Trump such a hateful person he has no moral or respect in a debate he blames everyone except him.\n##              id likes                  debate\n## 3  Dave Garland    99 abc_first_debate_manual\n## 7   Connie Sage    79 abc_first_debate_manual\n## 9  Susan Weyant    53 abc_first_debate_manual\n## 11  Lynn Kohler    41 abc_first_debate_manual\n## 12     Jim Lape    28 abc_first_debate_manual\n## 23   Joe Sonera    65 abc_first_debate_manual\n##    docname Segment       WPS WC Sixltr   Dic\n## 3    text3       3  6.333333 19  15.79 36.84\n## 7    text7       7 11.500000 23  17.39 17.39\n## 9    text9       9 15.333333 46   8.70 13.04\n## 11  text11      11 32.000000 32   6.25 28.12\n## 12  text12      12 13.000000 39  12.82  5.13\n## 23  text23      23 20.000000 20  15.00 25.00\n##    anger  fear  joy sadness AllPunc Period\n## 3   5.26 15.79 5.26   10.53   15.79   0.00\n## 7   4.35  4.35 0.00    8.70    8.70   0.00\n## 9   4.35  2.17 2.17    4.35   23.91  17.39\n## 11  9.38  6.25 3.12    9.38    9.38   9.38\n## 12  2.56  2.56 0.00    0.00   15.38   5.13\n## 23 10.00  5.00 5.00    5.00    5.00   5.00\n##    Comma Colon SemiC QMark Exclam Dash Quote\n## 3   0.00     0     0  0.00  15.79    0  0.00\n## 7   0.00     0     0  0.00   8.70    0  0.00\n## 9   4.35     0     0  2.17   0.00    0  0.00\n## 11  0.00     0     0  0.00   0.00    0  0.00\n## 12  2.56     0     0  0.00   2.56    0  5.13\n## 23  0.00     0     0  0.00   0.00    0  0.00\n##    Apostro Parenth OtherP\n## 3     0.00       0  15.79\n## 7     0.00       0   8.70\n## 9     0.00       0  23.91\n## 11    0.00       0   9.38\n## 12    5.13       0  10.26\n## 23    0.00       0   5.00"},{"path":"week3.html","id":"assignments-1---due-date-eod-friday-week-4-1","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.6 Assignments 1 - Due Date: EOD Friday Week 4","text":"Replicate results left-column Figure 3 Ventura et al. (2021).Look keywords context Biden ventura_etal_df dataset, compare results data, pre-processed (.e., lower-case, remove stopwords, etc.). version provides information context Biden appears comments?Use different collocation approach ventura_etal_df dataset, pre-process data (.e., lower-case, remove stopwords, etc.). approach (pre-processed pre-processed) provides better picture corpus collocations found?Compare positive sentiment comments mentioning trump comments mentioning biden using bing afinn. Note afinn gives numeric value, need choose threshold determine positive sentiment.Using bing, compare sentiment comments mentioning trump comments mentioning biden using different metrics (e.g., Young Soroka 2012, Martins Baumard 2020, Ventura et al. 2021).Create domain-specific dictionary apply ventura_etal_df dataset. Show limitations dictionary (e.g., false positives), comment much problem wanted conduct analysis corpus.","code":""},{"path":"week4.html","id":"week4","chapter":"4 Week 4: Complexity and Similarity","heading":"4 Week 4: Complexity and Similarity","text":"","code":""},{"path":"week4.html","id":"slides-3","chapter":"4 Week 4: Complexity and Similarity","heading":"Slides","text":"5 Complexity Similarity (link)","code":""},{"path":"week4.html","id":"setup-3","chapter":"4 Week 4: Complexity and Similarity","heading":"4.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(textdata) # text datasets\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(wesanderson) # to prettify\nlibrary(stringdist) # measure string distance\nlibrary(reshape2)"},{"path":"week4.html","id":"replicating-the-lecture","chapter":"4 Week 4: Complexity and Similarity","heading":"4.2 Replicating the Lecture","text":"weeks lecture, learned similarity complexity measures word- document-level. follow order lecture slides.","code":""},{"path":"week4.html","id":"comparing-text","chapter":"4 Week 4: Complexity and Similarity","heading":"4.3 Comparing Text","text":"different ways compare text, depending unit analysis:\n- Character-level comparisons\n- Token-level comparison","code":""},{"path":"week4.html","id":"character-level-comparisons","chapter":"4 Week 4: Complexity and Similarity","heading":"4.3.1 Character-Level Comparisons:","text":"Let’s start using character-level comparison tools evaluate two documents (case, two statements made given Ontario winter day):?stringdist, know “longest common substring distance defined number unpaired characters. distance equivalent edit distance allowing deletions insertions, weight one.” also learned Levenshtein distance Jaro distance. can easily implement using stringdist function:distance provides slightly different information relation documents. distances stringdist function can compute. something interests , information measure paper.ever used measure work? Actually, yes. combining corpus legislative speeches Ecuadorian Congress data set Ecuadorian legislators, matched names data set using fuzzy matching matching names closely related (even perfect match). example code:saved lot time. still needed validate matches manually match unmatched names.","code":"\ndoc_1 <- \"By the professor’s standards, the weather in Ontario during the Winter term is miserable.\"\ndoc_2 <- \"By the professor’s high standards, the weather in London during the Winter term is depressive.\"\nstringdist(doc_1,doc_2,method = \"lcs\")## [1] 27\nstringdist(doc_1,doc_2,method = \"lv\")## [1] 20\nstringdist(doc_1,doc_2,method = \"jw\")## [1] 0.1768849\n# Goal:\n# - You have two data frames, `df_a` and `df_b`.\n# - For each name in `df_a$CANDIDATO_to_MATCH`, you want to find the *closest* name in\n#   `df_b$CANDIDATO_MERGE` using Jaro–Winkler *distance* (\"jw\").\n# - Because this is a distance, *lower is better* (0 means identical).\n# - You treat it as a match when:\n#     (1) the *minimum* distance is below a threshold (0.4), AND\n#     (2) the best (minimum-distance) match is unique (i.e., no ties for the min).\n\nfor (i in 1:length(df_a$CANDIDATO_to_MATCH)) {\n\n  # Compute Jaro–Winkler distances from the current df_a name to *all* df_b candidate names\n  score_temp <- stringdist(\n    df_a$CANDIDATO_to_MATCH[i],\n    df_b$CANDIDATO_MERGE,\n    method = \"jw\"\n  )\n\n  # Identify the best match as the *minimum* distance\n  best_score <- min(score_temp, na.rm = TRUE)\n  best_idx   <- which(score_temp == best_score)\n\n  # Accept the match only if:\n  # - the best (minimum) distance is below the threshold, AND\n  # - there is exactly one best match (no ties)\n  if (best_score < 0.4 & length(best_idx) == 1) {\n\n    # Assign the matched name from df_b into the merge column of df_a\n    df_a$CANDIDATO_MERGE[i] <- df_b$CANDIDATO_MERGE[best_idx]\n\n  } else {\n\n    # Otherwise, record NA (no confident/unique match)\n    df_a$CANDIDATO_MERGE[i] <- NA\n  }\n}"},{"path":"week4.html","id":"token-level-comparisons","chapter":"4 Week 4: Complexity and Similarity","heading":"4.3.2 Token-Level Comparisons:","text":"compare documents token level (.e., many tokens coincide often), can think document row matrix word column. called document-feature matrices, dfms. using quanteda, first need tokenize corpus:Now ready create dfm:Just matrix (actually, sparse matrix becomes even sparser corpus grows). Now can measure similarity distance two texts. straightforward approach correlate occurrence 1s 0s across texts. One intuitive way see means transpose dfm present shape may find familiar:Ok, now just use simple correlation test:, can see text1 highly correlated text2 text3. Alternatively, can use built-functions quanteda obtain similar results without transforming dfm:can use textstat_simil whole bunch similarity/distance methods:Cosine similarity kind important method know advance course. can check app get visual example cosine similarity, app get visual example cosine similarity applied text data.can also present matrices nice plots:Noise!","code":"\ndoc_3 <- \"The professor has strong evidence that the weather in London (Ontario) is miserable and depressive.\"\n\ndocs_toks <- tokens(rbind(doc_1,doc_2,doc_3),\n                            remove_punct = T)\ndocs_toks <- tokens_remove(docs_toks,\n                           stopwords(language = \"en\"))\ndocs_toks## Tokens consisting of 3 documents.\n## text1 :\n## [1] \"professor’s\" \"standards\"   \"weather\"    \n## [4] \"Ontario\"     \"Winter\"      \"term\"       \n## [7] \"miserable\"  \n## \n## text2 :\n## [1] \"professor’s\" \"high\"        \"standards\"  \n## [4] \"weather\"     \"London\"      \"Winter\"     \n## [7] \"term\"        \"depressive\" \n## \n## text3 :\n## [1] \"professor\"  \"strong\"     \"evidence\"  \n## [4] \"weather\"    \"London\"     \"Ontario\"   \n## [7] \"miserable\"  \"depressive\"\ndocs_dmf <- dfm(docs_toks)\ndocs_dmf## Document-feature matrix of: 3 documents, 13 features (41.03% sparse) and 0 docvars.\n##        features\n## docs    professor’s standards weather ontario\n##   text1           1         1       1       1\n##   text2           1         1       1       0\n##   text3           0         0       1       1\n##        features\n## docs    winter term miserable high london\n##   text1      1    1         1    0      0\n##   text2      1    1         0    1      1\n##   text3      0    0         1    0      1\n##        features\n## docs    depressive\n##   text1          0\n##   text2          1\n##   text3          1\n## [ reached max_nfeat ... 3 more features ]\ndfm_df <- convert(docs_dmf, to = \"matrix\")\ndfm_df_t <- t(dfm_df)\ndfm_df_t##              docs\n## features      text1 text2 text3\n##   professor’s     1     1     0\n##   standards       1     1     0\n##   weather         1     1     1\n##   ontario         1     0     1\n##   winter          1     1     0\n##   term            1     1     0\n##   miserable       1     0     1\n##   high            0     1     0\n##   london          0     1     1\n##   depressive      0     1     1\n##   professor       0     0     1\n##   strong          0     0     1\n##   evidence        0     0     1\ncor(dfm_df_t[,c(1:3)])##            text1      text2      text3\n## text1  1.0000000  0.2195775 -0.4147575\n## text2  0.2195775  1.0000000 -0.6250000\n## text3 -0.4147575 -0.6250000  1.0000000\ntextstat_simil(docs_dmf, margin = \"documents\", method = \"correlation\")## textstat_simil object; method = \"correlation\"\n##        text1  text2  text3\n## text1  1.000  0.220 -0.415\n## text2  0.220  1.000 -0.625\n## text3 -0.415 -0.625  1.000\ntextstat_simil(docs_dmf, margin = \"documents\", method = \"cosine\")## textstat_simil object; method = \"cosine\"\n##       text1 text2 text3\n## text1 1.000 0.668 0.401\n## text2 0.668 1.000 0.375\n## text3 0.401 0.375 1.000\ntextstat_simil(docs_dmf, margin = \"documents\", method = \"jaccard\")## textstat_simil object; method = \"jaccard\"\n##       text1 text2 text3\n## text1  1.00 0.500 0.250\n## text2  0.50 1.000 0.231\n## text3  0.25 0.231 1.000\ntextstat_dist(docs_dmf, margin = \"documents\", method = \"euclidean\")## textstat_dist object; method = \"euclidean\"\n##       text1 text2 text3\n## text1     0  2.24  3.00\n## text2  2.24     0  3.16\n## text3  3.00  3.16     0\ntextstat_dist(docs_dmf, margin = \"documents\", method = \"manhattan\")## textstat_dist object; method = \"manhattan\"\n##       text1 text2 text3\n## text1     0     5     9\n## text2     5     0    10\n## text3     9    10     0\ncos_sim_doc <- textstat_simil(docs_dmf, margin = \"documents\", method = \"cosine\")\ncos_sim_doc <- as.matrix(cos_sim_doc)\n  \n# We do this to use ggplot\ncos_sim_doc_df <- as.data.frame(cos_sim_doc)\ncos_sim_doc_df %>%\n    rownames_to_column() %>%\n  # ggplot prefers \n    melt() %>%\n    ggplot(aes(x = as.character(variable),y = as.character(rowname), col = value)) +\n    geom_tile(col=\"black\", fill=\"white\") + \n    # coord_fixed() +\n    labs(x=\"\",y=\"\",col = \"Cosine Sim\", fill=\"\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(\n      angle = 90,\n      vjust = 1,\n      hjust = 1)) +\n    geom_point(aes(size = value)) + \n    scale_size(guide = 'none') +\n    scale_color_gradient2(mid=\"#A63446\",low= \"#A63446\",high=\"#0C6291\") +\n    scale_x_discrete(expand=c(0,0)) +\n    scale_y_discrete(expand=c(0,0))## Using rowname as id variables"},{"path":"week4.html","id":"complexity","chapter":"4 Week 4: Complexity and Similarity","heading":"4.4 Complexity","text":"week’s lecture (one readings), know another way analyzing text computing complexity. Schoonvelde et al. (2019), “Liberals Lecture, Conservatives Communicate: Analyzing Complexity Ideology 381,609 Political Speeches,” authors use Flesch’s Reading Ease Score measure “complexity,” readability (see ??textstat_readability formula readability measures). Flesch’s Reading Ease Score ranges 0 100, higher values indicate less complex (readable) text. example, score 90 100 suggests text can understood 5th grader; score 0 30 suggests text typically understandable college graduates professionals. score computed using average sentence length, number words, number syllables.Let’s apply readability score open-ended questions 2020 ANES survey, see correlate characteristics respondents.open-ended survey questions ask respondents like dislike Democratic (Joe Biden) Republican (Donald Trump) 2020 U.S. presidential candidates election. Note survey respondents opt question assigned NA.Let’s check:Makes sense: third row quite easy ready, fourth row bit complex, eleventh row impossible read Spanish.Look … degree makes speak complicated.","code":"\nload(\"data/anes_sample.Rdata\")\nhead(open_srvy)## # A tibble: 6 × 9\n##   V200001 like_dem_pres       dislike_dem_pres\n##     <dbl> <chr>               <chr>           \n## 1  200015 <NA>                nothing about s…\n## 2  200022 <NA>                He wants to tak…\n## 3  200039 He is not Donald T… <NA>            \n## 4  200046 he look honest and… <NA>            \n## 5  200053 <NA>                Open borders, l…\n## 6  200060 he is NOT Donald T… <NA>            \n## # ℹ 6 more variables: like_rep_pres <chr>,\n## #   dislike_rep_pres <chr>, income <int>,\n## #   pid <int>, edu <int>, age <int>\nread_like_dem_pres <- textstat_readability(open_srvy$like_dem_pres,measure = \"Flesch\")\nopen_srvy$read_like_dem_pres <- read_like_dem_pres$Flesch\nhead(open_srvy[,c(2,10)],15)## # A tibble: 15 × 2\n##    like_dem_pres            read_like_dem_pres\n##    <chr>                                 <dbl>\n##  1 <NA>                                   NA  \n##  2 <NA>                                   NA  \n##  3 He is not Donald Trump.               100. \n##  4 he look honest and his …               54.7\n##  5 <NA>                                   NA  \n##  6 he is NOT Donald Trump …              100. \n##  7 he has been in gov for …               89.6\n##  8 <NA>                                   NA  \n##  9 he is wanting to do thi…               96.0\n## 10 <NA>                                   NA  \n## 11 Candidato adecuado para…              -10.8\n## 12 <NA>                                   NA  \n## 13 <NA>                                   NA  \n## 14 Everything he stands fo…               75.9\n## 15 He is very intuned with…               81.9\nopen_srvy %>%\n  # Remove people who did not answer\n  filter(edu>0) %>%\n  # Remove negative scores\n  filter(read_like_dem_pres>0) %>%\n  ggplot(aes(x=as.factor(edu),y=read_like_dem_pres)) +\n  geom_boxplot(alpha=0.6) +\n  # scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  # scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(x=\"Education\", y = \"Flesch Score\", \n       caption = \"Note: Education goes from 1 - Less the high school credentials to 5 - Graduate Degree\")"},{"path":"week4.html","id":"exercise-optional-1","chapter":"4 Week 4: Complexity and Similarity","heading":"4.5 Exercise (Optional)","text":"Extend analysis ANES data using readiability scores /variables. Alternatively, use surveys open-ended questions (e.g., CES).wanted use similarity/distance measure explore ANES/CES open-ended responses, go ? able compare using data provided?","code":""}]
